{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1f-GGs5LnFNYFh-QAL9pcu3Za7MxsxKS2","timestamp":1768297560204}]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","source":["#### Prerequisites\n","\n","- Basic familiarity with [Numpy](https://numpy.org/doc/stable/user/quickstart.html)\n","- Basic familiarity with [Pyplot](https://matplotlib.org/stable/tutorials/introductory/pyplot.html)\n","- Basic familiarity with [Pandas](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html)\n","\n","\n","## Outline\n","\n","<a name=\"outline\"></a>\n","\n","- [Section 1](#section-1): Decision Tree Classifier\n","  - [(1.a)](#section-1a) Dataset Preparation\n","  - [(1.b)](#section-1b) Intro to Decision Trees and GINI-index\n","  - [(1.c)](#section-1c) Decision Tree Training\n","  - [(1.d)](#section-1d) Decision Tree Classfication Algorithm\n","- [Section 2](#section-2): From Decision Tree to Random Forest Algorithm\n","  - [(2.a)](#section-2a) Intro to Random Forest\n","  - [(2.b)](#section-2b) Training: Feature Bagging\n","  - [(2.c)](#section-2c) Training: Bootstrapping\n","  - [(2.d)](#section-2d) Classification: Aggregation\n"],"metadata":{"id":"VelgvQCYQjVN"}},{"cell_type":"markdown","metadata":{"id":"rcr6esW4Fv6_"},"source":["<a name=\"section-1\"></a>\n","\n","# Section 1: Decision Tree Classifier\n","Decision trees are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of the label of unseen data by learning simple decision rules inferred from the data features.\n","\n","You can understand it as using a set of _if-then-else_ decision rules, e.g., _if_ it snowed in London, _then_ many Londoners would ski on Primrose Hill. _Else_, they would walk in Hyde Park.\n","Generally speaking, the deeper the tree, i.e., the more _if-then-else_ decisions are subsequently made in our model, the more complex the decision rules and the fitter the model. However, note that decision trees are prone to overfitting.\n"]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Initial global configuration for matplotlib\n","SMALL_SIZE = 12\n","MEDIUM_SIZE = 16\n","BIGGER_SIZE = 20\n","\n","plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n","plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n","plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n","plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n","plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n","plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n","plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n","\n","# fixing random generator for reproducibility\n","rng = np.random.default_rng(0)"],"metadata":{"id":"oZznZxN9qP0L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","\n","<a name=\"section-1a\"></a>\n","\n","## (1a) Dataset Preparation [^](#outline)\n","\n","In this notebook, we will use decision trees as a classification algorithm with the GINI-index, and work with the famous [Iris data set](https://en.wikipedia.org/wiki/Iris_flower_data_set). It contains four biological characteristics _(features)_ of 150 samples that belong to three species _(classes)_ of the family of Iris flowers (Iris setosa, Iris virginica and Iris versicolor). The data set provides 50 samples for each species."],"metadata":{"id":"oa5KlLwlCQUE"}},{"cell_type":"code","source":["from google.colab import files\n","\n","iris_data = files.upload()"],"metadata":{"id":"rJcrXPmoNNkC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the data\n","X_y = pd.read_csv('iris.csv')"],"metadata":{"id":"5N-6mSUSNZ6h"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jWCJ9D5PhRQr"},"source":["# check\n","X_y.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uMp0Q3eEhyFi"},"source":["It is always a good idea to see whether the features are correlated. The python package `seaborn` has a nice one-line command called `pairplot` to explore this visually. It directly prints the feature names (sepal length, sepal width, petal length, petal width) and labels as axis titles. The `pairplot` plot below is used to visualize the distribution across samples of each descriptor and the correlation between descriptors (this helps identify collinear features)."]},{"cell_type":"code","metadata":{"id":"wQsi2D6ThtKU"},"source":["import seaborn as sns\n","\n","sns.pairplot(X_y);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can observe some pairs of features are correlated with various degrees while others are not. We specifically observe that *petal width* and *petal length* are strongly correlated, while *petal width* is correlated *sepal width* with a lesser degree. We also observe that labels are balanced in this dataset."],"metadata":{"id":"0E-cfsH6rg9y"}},{"cell_type":"markdown","metadata":{"id":"0UjOk2mjjooA"},"source":["As with any other supervised machine learning method, we create a train and test set to learn and evaluate our model, respectively."]},{"cell_type":"code","metadata":{"id":"QTMU873Dh_G-"},"source":["# shuffling the data\n","X_y_shuff = X_y.iloc[rng.permutation(len(X_y))]\n","\n","# we split train to test as 70:30\n","split_rate = 0.7\n","split_idx = int(split_rate*(X_y_shuff.shape[0]))\n","train, test = X_y_shuff.iloc[:split_idx], X_y_shuff.iloc[split_idx:]\n","\n","# Separate the training features into X_train by extracting all columns except the last one,\n","# which represent the labels, and vice versa for y_train.\n","X_train = train[train.columns[:-1]]\n","y_train = train[train.columns[-1]]\n","\n","# Simlarly to test split.\n","X_test = test[test.columns[:-1]]\n","y_test = test[test.columns[-1]]\n","\n","y_train = y_train.astype(int)\n","y_test = y_test.astype(int)\n","\n","# We assume all examples have the same weight.\n","training_weights = np.ones_like(y_train) / len(y_train)\n","\n","# We need a dictionary indicating whether the column index maps to a\n","# categorical feature or numerical\n","# In this example, all features are numerical (categorical=False)\n","columns_dict = {index: False for index in range(X_train.shape[1])}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RD7QG1voKIkF"},"source":["We will build up our decision tree algorithm in the pythonic way how we have done it previously as well by calling functions we define first in functions we define later. For quick evaluations of your implementations, however, we have also included one-line commands after each cell to see whether your implementation results in errors.\n","\n","\n","<a name=\"section-1b\"></a>\n","\n","\n","\n","## (1b) Intro to Decision Trees and GINI-index [^](#outline)\n","\n","Our goal is to predict for each data point $\\boldsymbol x = (x_1,\\ldots,x_p)$ a label $y \\in \\mathcal{C}_q=\\{c_1,\\ldots,c_Q\\}$ where we have in total $Q$ classes.\n","\n","Recall that each split $(j,s)$ split the space of feature $x_j$ into two regions $R_1(j,s)$ and $R_2(j,s)$. **If the feature is a continuous variable**, the $s$ is used as a threshold such that samples with feature value less than $s$ are assigned to the left, and vice versa. **if the feature is a categorical variable**, the $s$ is used as a predicate such that samples with feature value equals to $s$ are assigned to left, and vice versa.\n","\n","For a region $R_\\alpha$ and a class $c_q$, we define the probability of being in that region and belonging to that class by\n","$$  \\pi_q(R_{\\alpha}) = \\frac{\\sum_{i=1}^N I\\big( \\boldsymbol x^{(i)} \\in R_{\\alpha} \\land y^{(i)} \\in c_q \\big)}{\\sum_{i=1}^N I(\\boldsymbol x^{(i)} \\in R_{\\alpha})}$$\n","and we define the $Q\\times 1$ vector $\\boldsymbol \\pi_q(R_{\\alpha})$ whose $q$-th entry is $\\pi_q(R_{\\alpha})$.\n","\n","For a general $\\boldsymbol p = (p_1,\\ldots,p_Q)^\\top$, the Gini index $\\text{GI}(\\boldsymbol p)$ is defined as: $\\text{GI}(\\boldsymbol p) = 1 - \\sum_{i=1}^Q p_i^2 $. To determine the best $j$ and $s$ at each split node, we may use _GINI-index_ to search for $j$ and $s$ that minimizes the weighted sum of _GINI-index_ of the left side samples and right side samples:\n","  \n","  $$\\text{GI}(\\boldsymbol \\pi; j, s) = p_{R_1} \\text{GI}(\\boldsymbol \\pi(R_1); j, s) + p_{R_2} \\text{GI}(\\boldsymbol \\pi(R_2); j, s)$$\n","\n","where $p_{R_1}$ and $p_{R_2}$ are, respectively, the fraction of the number of samples on the left and on the right and $R_1$ and $R_2$ are determined by $(j,s)$.\n","\n","It's your turn to implement it in the next cell. We want to allow the code to consider samples with different weights, hence introduce an additional argument called `sample_weights`. The Iris data set we work with in this notebook has uniform sample weights, but other data sets you work with in the future could be different."]},{"cell_type":"code","source":["# EDIT THIS FUNCTION\n","def gini_index(y):\n","    \"\"\"\n","    Arguments:\n","        y: vector of training labels, of shape (N,).\n","    Returns:\n","        (float): the GINI-index for the labels in a given region.\n","    \"\"\"\n","\n","    # count different labels in yï¼Œand store in p\n","    # initialize with zero for each distinct label.\n","    p = {yi: 0 for yi in set(y)}\n","    for yi in y:\n","        p[yi] += ## <-- EDIT THIS LINE\n","\n","    sum_p_squared = 0\n","    for yi in p:\n","        sum_p_squared += ## <-- EDIT THIS LINE\n","\n","    # Return GINI-Index\n","    return ## <-- EDIT THIS LINE"],"metadata":{"id":"vyXEN6wWRIH_"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1OoieamNKKjJ"},"source":["# evaluate labels y\n","print('Gini index: ',gini_index(y_train.to_numpy()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check you result (Warning: use only if you run the cells sequentially from the beginning)\n","gini_index(y_train.to_numpy())==0.6659410430839001"],"metadata":{"id":"EUHRoHCy3Vwq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ByxQZ5BHOJIN"},"source":["<a name=\"section-1c\"></a>\n","\n","\n","\n","## (1c) Decision Tree Training [^](#outline)\n","\n","\n","Next, we define a function to split the data samples based on a feature (column) index and a value (recall how this value will be used whether the feature is categorical or numerical).\n","\n","This has not much use yet, but we will call it in later functions, e.g., in the next cell in `gini_split_value`."]},{"cell_type":"code","metadata":{"id":"bd8WFZLKF6iT"},"source":["# EDIT THIS FUNCTION\n","\n","def split_samples(X, y, column, value, categorical):\n","    \"\"\"\n","    Return the split of data whose column-th feature:\n","      1. equals value, in case `column` is categorical, or\n","      2. less than value, in case `column` is not categorical (i.e. numerical)\n","\n","    Arguments:\n","        X: training features, of shape (N, p).\n","        y: vector of training labels, of shape (N,).\n","        column: the column of the feature for splitting.\n","        value: splitting threshold  the samples\n","        categorical: boolean value indicating whether column is a categorical variable or numerical.\n","    Returns:\n","        tuple(np.array, np.array, np.array): tuple of the left split data (X_l, y_l).\n","        tuple(np.array, np.array, np.array): tuple of the right split data (X_l, y_l)\n","    \"\"\"\n","\n","    if categorical:\n","        left_mask =(X[:, column] == value)\n","    else:\n","        left_mask = (X[:, column] < value)\n","\n","    # Using the binary masks `left_mask`, we split X, y, and sample_weights.\n","    X_l, y_l ## <-- EDIT THIS LINE\n","    X_r, y_r ## <-- EDIT THIS LINE\n","\n","    return (X_l, y_l), (X_r, y_r)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6EMkU0OTPGJ5"},"source":["For a given feature index, we need to estimate the best value $s$ to use as threshold (for numerical variables) or predicate (for categorical variables). We need to implement the function the searches for $s$ that minimizes the _GINI-index_. Let's do this in the following cell by calling our previously defined two functions `split_samples` and `gini_index`."]},{"cell_type":"code","source":["# EDIT THIS FUNCTION\n","def gini_split_value(X, y, column, categorical):\n","    \"\"\"\n","    Calculate the GINI-index based on `column` with the split that minimizes the GINI-index.\n","    Arguments:\n","        X: training features, of shape (N, p).\n","        y: vector of training labels, of shape (N,).\n","        column: the column index of the feature for calculating. 0 <= column < p\n","        categorical: boolean value indicating whether column is a categorical variable or numerical.\n","    Returns:\n","        (float, float): the resulted GINI-index and the corresponding value used in splitting.\n","    \"\"\"\n","\n","    unique_vals = np.unique(X[:, column])\n","\n","    assert len(unique_vals) > 1, f\"There must be more than one distinct feature value. Given: {unique_vals}.\"\n","\n","    gini_index_val, threshold = np.inf, None\n","\n","    # split the values of i-th feature and calculate the cost\n","    for value in unique_vals:\n","        (X_l, y_l), (X_r, y_r) = ## <-- EDIT THIS LINE\n","\n","        # if one of the two sides is empty, skip this split.\n","        if len(y_l) == 0 or len(y_r) == 0:\n","            continue\n","\n","        p_left = len(y_l)/(len(y_l)+len(y_r))\n","        p_right = len(y_r)/(len(y_l)+len(y_r))\n","\n","        # compute the GINI index for the left and right subsets\n","        new_cost = ## <-- EDIT THIS LINE\n","\n","        # update the minimum GINI index and the threshold if this split is better\n","        if new_cost < gini_index_val:\n","              gini_index_val, threshold = new_cost, value\n","\n","    return gini_index_val, threshold"],"metadata":{"id":"0X87xfpLaNur"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3fWSvdhqP0iY"},"source":["# evaluate for feature sepal width (cm)\n","gini_split_value(X_train.to_numpy(), y_train.to_numpy(), 3, columns_dict[3])"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check you result (Warning: use only if you run the cells sequentially from the beginning)\n","gini_split_value(X_train.to_numpy(), y_train.to_numpy(), 3, columns_dict[3])==(0.3235294117647065, 1.0)"],"metadata":{"id":"g3_JjDUFQq4L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rVkI3wBVQ12B"},"source":["It's now time to choose the best feature to split by calling the function `gini_split_value` for each feature."]},{"cell_type":"code","source":["## EDIT THIS FUNCTION\n","def gini_split(X, y, columns_dict):\n","    \"\"\"\n","    Choose the best feature to split according to criterion.\n","    Args:\n","        X: training features, of shape (N, p).\n","        y: vector of training labels, of shape (N,).\n","        columns_dict: a dictionary mapping column indices to whether the column is categorical or numerical variable.\n","    Returns:\n","        (int, float): the best feature index and value used in splitting.\n","        If the feature index is None, then no valid split for the current Node.\n","    \"\"\"\n","\n","    # Initialize `split_column` to None, so if None returned this means there is no valid split at the current node.\n","    min_gini_index = np.inf\n","    split_column = None\n","    split_val = np.nan\n","\n","    for column, categorical in columns_dict.items():\n","        # skip column if samples are not separable by that column.\n","        if len(np.unique(X[:, column])) < 2:\n","            continue\n","        gini_index, current_split_val = ## <-- EDIT THIS LINE\n","\n","\n","        if ## <-- EDIT THIS LINE\n","            # Keep track with:\n","\n","            # 1. the current minimum gini-index value,\n","            min_gini_index = ## <-- EDIT THIS LINE\n","\n","            # 2. corresponding column,\n","            split_column = ## <-- EDIT THIS LINE\n","\n","            # 3. corresponding split threshold.\n","            split_val = ## <-- EDIT THIS LINE\n","\n","    return min_gini_index, split_column, split_val"],"metadata":{"id":"dMHkUVxedtjc"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mxY8uqmHRc-N"},"source":["# evaluate which feature is best\n","gini_split(X_train.to_numpy(), y_train.to_numpy(), columns_dict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check you result (Warning: use only if you run the cells sequentially from the beginning)\n","gini_split(X_train.to_numpy(), y_train.to_numpy(), columns_dict)==(0.3235294117647065, 2, 3.0)"],"metadata":{"id":"BrAbN1H1QnQT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"trLa8CV7SF2I"},"source":["Now, we need a function that computes $\\pi$, the vector of class probabilities, for a given set of labels $y$."]},{"cell_type":"code","source":["from collections import defaultdict\n","\n","def impurity(y):\n","    \"\"\"\n","    Return the class probabilities.\n","    Args:\n","        y: vector of training labels, of shape (N,).\n","    Returns:\n","        (dict): dict of class probabilities\n","    \"\"\"\n","    count = defaultdict(int)\n","\n","    for yi in y:\n","        count[yi] += 1 / len(y)\n","\n","    return count"],"metadata":{"id":"FbTFFixEgtuk"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fZzFMoQtS4tR"},"source":["# evaluate it\n","impurity(y_train.to_numpy())"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check you result (Warning: use only if you run the cells sequentially from the beginning)\n","impurity(y_train.to_numpy()) == {0: 0.3523809523809525, 1: 0.31428571428571433, 2: 0.3333333333333334}"],"metadata":{"id":"tkUp4hqtQ09L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nHihzl6CS9X0"},"source":["Finally, we can build the decision tree by using `choose_best_feature` to find the best feature to split the `X`, and `split_dataset` to get sub-trees.\n","\n","Note: If you are not familiar with recursion, check the following tutorial [Python Recursion](https://www.datamentor.io/python/recursive-function)."]},{"cell_type":"code","metadata":{"id":"fxsU-BWeEZRu"},"source":["# EDIT THIS FUNCTION\n","def build_tree(X, y, columns_dict, feature_names, depth, max_depth=10, min_samples_leaf=2):\n","    \"\"\"Build the decision tree according to the data.\n","    Args:\n","        X: (np.array) training features, of shape (N, p).\n","        y: (np.array) vector of training labels, of shape (N,).\n","        columns_dict: a dictionary mapping column indices to whether the column is categorical or numerical variable.\n","        feature_names (list): record the name of features in X in the original dataset.\n","        depth (int): current depth for this node.\n","    Returns:\n","        (dict): a dict denoting the decision tree (binary-tree). Each parent node has seven attributes:\n","          1. 'feature_name': The column name of the split.\n","          2. 'feature_index': The column index of the split.\n","          3. 'value': The value used for the split.\n","          4. 'number of data points': The number of data points in this node.\n","          5. 'Gini-index': The GINI-index of this node.\n","          6. 'categorical': indicator for categorical/numerical variables.\n","          7. 'class_probabilities': For leaf nodes, this stores ALL class probabilities. Otherwise, it is None.\n","          8. 'left': The left sub-tree with the same structure.\n","          9. 'right' The right sub-tree with the same structure.\n","    \"\"\"\n","    # include a clause for the cases where (i) no feature, (ii) all labels are the same, (iii) depth exceeded, or (iv) X is too small\n","    if len(np.unique(y))==1 or depth>max_depth or len(X)<=min_samples_leaf:\n","        return {'number of data points': len(y), 'class_probabilities': impurity(y)}\n","\n","    GI, split_index, split_val = ## <-- EDIT THIS LINE\n","\n","    # If no valid split at this node, use majority vote.\n","    if split_index is None:\n","        return {'number of data points': len(y), 'class_probabilities': impurity(y)}\n","\n","    categorical = columns_dict[split_index]\n","\n","    # Split samples (X, y, sample_weights) given column, split-value, and categorical flag.\n","    (X_l, y_l), (X_r, y_r) = ## <-- EDIT THIS LINE\n","    return {\n","        'feature_name': feature_names[split_index],\n","        'feature_index': split_index,\n","        'value': split_val,\n","        'number of data points': len(y),\n","        'Gini-index': GI,\n","        'categorical': categorical,\n","        'class_probabilities': None,\n","        'left': build_tree(X_l, y_l, columns_dict, feature_names, depth + 1, max_depth, min_samples_leaf),\n","        'right': build_tree(X_r, y_r, columns_dict, feature_names, depth + 1, max_depth, min_samples_leaf)\n","    }"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9VZ5LpqAUqaa"},"source":["We define a wrapper function that we call `train` to call this `build_tree` function with the appropriate arguments."]},{"cell_type":"code","metadata":{"id":"VwqMWBa3EZOW"},"source":["def train(X, y,  columns_dict, max_depth=10):\n","    \"\"\"\n","    Build the decision tree according to the training data.\n","    Args:\n","        X: (pd.Dataframe) training features, of shape (N, p). Each X[i] is a training sample.\n","        y: (pd.Series) vector of training labels, of shape (N,). y[i] is the label for X[i], and each y[i] is\n","        an integer in the range 0 <= y[i] <= C. Here C = 1.\n","        columns_dict: a dictionary mapping column indices to whether the column is categorical or numerical variable.\n","        sample_weights: weights for each samples, of shape (N,).\n","    \"\"\"\n","\n","    feature_names = X.columns.tolist()\n","    X = X.to_numpy()\n","    y = y.to_numpy()\n","    return build_tree(X, y, columns_dict, feature_names, depth=1, max_depth=max_depth)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check you result (Warning: use only if you run the cells sequentially from the beginning)\n","tree0 = train(X_train, y_train, columns_dict,max_depth=2)\n","tree0 == {'feature_name': 'petal length (cm)',\n"," 'feature_index': 2,\n"," 'value': np.float64(3.0),\n"," 'number of data points': 105,\n"," 'Gini-index': 0.3235294117647065,\n"," 'categorical': False,\n"," 'class_probabilities': None,\n"," 'left': {'number of data points': 37,\n","  'class_probabilities': defaultdict(int, {np.int64(0): 0.9999999999999991})},\n"," 'right': {'feature_name': 'petal length (cm)',\n","  'feature_index': 2,\n","  'value': np.float64(5.0),\n","  'number of data points': 68,\n","  'Gini-index': 0.10919276801629782,\n","  'categorical': False,\n","  'class_probabilities': None,\n","  'left': {'number of data points': 35,\n","   'class_probabilities': defaultdict(int,\n","               {np.int64(1): 0.9142857142857145,\n","                np.int64(2): 0.08571428571428572})},\n","  'right': {'number of data points': 33,\n","   'class_probabilities': defaultdict(int,\n","               {np.int64(2): 0.9696969696969691,\n","                np.int64(1): 0.030303030303030304})}}}"],"metadata":{"id":"Jf95Gr7nidrp"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U1T3PFpjEZLd"},"source":["# fit the decision tree with training data\n","tree = train(X_train, y_train, columns_dict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tree"],"metadata":{"id":"nlMbFn9Sox0C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bAE19BbTXYJb"},"source":["<a name=\"section-1d\"></a>\n","\n","## (1d) Decision Tree Classification Algorithm [^](#outline)\n","\n","\n","Now, we want to use this fitted decision tree to make predictions for our test set `X_test`. To do so, we first define a function `classify` that takes each single data point `x` as an argument. We will write a wrapper function `predict` that calls this `classify` function. In the following implementation, we will traverse a given decision tree basen on the current feature vector that we have. Traversing tree-like objects in programming is straightforwardly implemented using recursion. If you are not familiar with recursion, check the following tutorial [Python Recursion](https://www.datamentor.io/python/recursive-function).\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"QIJYH7hNWOd-"},"source":["# EDIT THIS FUNCTION\n","def classify(tree, x):\n","    \"\"\"\n","    Classify a single sample with the fitted decision tree.\n","    Args:\n","        x: ((pd.Dataframe) a single sample features, of shape (p,).\n","    Returns:\n","        (int): predicted testing sample label.\n","    \"\"\"\n","    if tree['class_probabilities'] is not None:\n","        # return the label with the highest probability\n","        return max(tree['class_probabilities'], key=tree['class_probabilities'].get)\n","\n","    elif tree['categorical']:\n","        if x[tree['feature_index']] == tree['value']:\n","            # go to left branch\n","            return classify(tree['left'], x)\n","        else:\n","            # go to right branch\n","            return classify(tree['right'], x)\n","\n","    else:\n","\n","        if x[tree['feature_index']] ## <-- EDIT THIS LINE\n","            # go to left branch\n","            return ## <-- EDIT THIS LINE\n","        else:\n","            # go to right branch\n","            return ## <-- EDIT THIS LINE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D_WNC05qWOYG"},"source":["def predict(tree, X):\n","    \"\"\"\n","    Predict classification results for X.\n","    Args:\n","        X: (pd.Dataframe) testing sample features, of shape (N, p).\n","    Returns:\n","        (np.array): predicted testing sample labels, of shape (N,).\n","    \"\"\"\n","    if len(X.shape) == 1:\n","        return classify(tree, X)\n","    else:\n","        return np.array([classify(tree, x) for x in X])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-yiJR4aQX2RI"},"source":["To evaluate how well the tree can generalise to unseen data in `X_test`, we define a short function that computes the mean accuracy."]},{"cell_type":"code","source":["## EDIT THIS FUNCTION\n","def tree_score(tree, X_test, y_test):\n","    y_pred = ## <-- EDIT THIS LINE\n","    return np.mean(y_pred==y_test)"],"metadata":{"id":"y-xx90u9qySr"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FX4nyXObWjI0"},"source":["print('Training accuracy:', tree_score(tree, X_train.to_numpy(), y_train.to_numpy()))\n","print('Test accuracy:', tree_score(tree, X_test.to_numpy(), y_test.to_numpy()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check you result (Warning: use only if you run the cells sequentially from the beginning)\n","tree_score(tree, X_train.to_numpy(), y_train.to_numpy())==0.9904761904761905"],"metadata":{"id":"p8OmFhy65C2c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jaWlcZAZYQrv"},"source":["## Questions on Decision Trees:\n","\n","1. What do the results above tell you? Has the decision tree you implemented overfitted to the training data?\n","2. If so, what can you change to counteract this problem?\n","3. Can you think of other information criteria than the GINI-index? If so, try implementing them and compare it to the decision tree with the GINI-index.\n","4. Recall ROC-curve (see Notebooks of Logistic Regression) that we used to compute the AUC as a more robust performance estimate, it required that each decision is associated with a probability (which can also be any kind of score representing the decision confidence). **Suggest** what can be used as score (or confidence) to be associated with each decision. Try to adapt for ROC-curve-compatible decisions and plot the ROC-curve indicating the AUC.\n","5. The computational cost of decision trees depends on the number of features. How to efficiently implement decision trees with high-dimensional datasets?\n","6. What do you need to modify if you want to give samples different weights?"]},{"cell_type":"markdown","source":["<a name=\"section-2\"></a>\n","\n","# Section 2: From Decision Tree to Random Forest [^](#outline)\n","\n","**To avoid confusion in the text here**:\n","  - **training-samples** used to mean the rows in `X_train` in other occasions, and we will use **training-examples** or **training-instances** instead to avoid confusion with\n","  - a **random-sample**, which means a selection of instances by chance from a group of instances.\n"],"metadata":{"id":"4QB29VGJsMOd"}},{"cell_type":"markdown","source":["\n","<a name=\"section-2a\"></a>\n","\n","## (2a) Intro to Random Forest [^](#outline)\n","\n","It is now the time to build ensemble model from individual models for the first time in this course, applied to ensemble of decision trees.\n","\n","\n","One approach for ensemble methods is [bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)). In particular we can apply bootstrapping for multiple decision trees upon on levels:\n","\n","1. Bootstrap on the training-instances: each model is trained on a random-sample (**with replacement**) from the training-instances. Eventually, as learned from lecture notes, decisions are aggregated across the models, hence the name _bagging_ (Bootstrap Aggregate).\n","\n","  Note that for each decision tree we randomly-sample from training-instances **with replacement** and the size of each random-sample is the same size of the training-instances. This will basically allow some training-instances to be duplicated and other instances to be excluded.\n","2. Feature bagging: at each split, a subset of features are considered before searching for the best split column $j$ and value $s$.\n","\n","\n","The image below illustrates random forest construction from individual decision trees. Notice the random-sampling that occurs at two levels:\n","\n","1. At red-stars: bootstrapping (random-sampling with replacement) of training-instances.\n","2. At blue-circles: random-sampling without replacement of features at each split node.\n","\n","![rf](https://raw.githubusercontent.com/barahona-research-group/mfds-resources/main/images/rf2.drawio.png)\n","\n","This design will leave us two hyperparameters for random forest:\n","\n","1. $B$: number of decision trees.\n","2. `n_features`: number of features (columns) sampled, **without replacement**, at each split before searching for the best split column $j$ and value $s$. For classification task `n_features` is recommended to be the number of all columns divided by 3.\n","3. Add to these the decision trees hyperparameters like `max_depth` and `min_leaf_sample`.\n","\n","\n","\n","\n","The second layer of sampling, i.e. _feature bagging_, requires us to re-implement the `gini_split` function to subsample from the feature columns before searching for the best split.\n","\n","\n","Modify the following function that will be employed for random forest decision trees to find the best split column $j$ (out from sampled `n_features` columns) and value $s$. Consider using the `gini_split_value` function that was already implemented for individual decision tree implementation."],"metadata":{"id":"fYFqIY-pCxhF"}},{"cell_type":"markdown","source":["<a name=\"section-2b\"></a>\n","\n","## (2b) Training: Feature Bagging Implementation [^](#outline)\n","\n","Remember that we need to sample `n_features` from the column at each split, before we scan for the optimal splitting column.  This will require us to make  adapted versions of two of the decision tree functions:\n","\n","1. `gini_split`: We need a new version `gini_split_rf` to add sampling step here before scanning for the optimal splitting column.\n","2. `build_tree`: because it depends on `gini_split`. We make another version `build_tree_rf` that calls `gini_split_rf`."],"metadata":{"id":"ZyQmhyLtDoTj"}},{"cell_type":"code","source":["## EDIT THIS FUNCTION\n","def gini_split_rf(n_features, X, y, columns_dict):\n","    \"\"\"\n","    Choose the best feature to split according to criterion.\n","    Args:\n","        n_features: number of sampled features.\n","        X: training features, of shape (N, p).\n","        y: vector of training labels, of shape (N,).\n","        columns_dict: a dictionary mapping column indices to whether the column is categorical or numerical variable.\n","    Returns:\n","        (float, int, float): the minimized gini-index, the best feature index and value used in splitting.\n","    \"\"\"\n","\n","    # The added sampling step.\n","    columns = rng.choice(list(columns_dict.keys()), n_features, replace=False)\n","    columns_dict = {c: columns_dict[c] for c in columns}\n","\n","    min_gini_index, split_column, split_val = np.inf, 0, 0\n","\n","    # Only scan through the sampled columns in `columns_dict`.\n","    for column, categorical in columns_dict.items():\n","        # skip column if samples are not separable by that column.\n","        if len(np.unique(X[:, column])) < 2:\n","            continue\n","\n","        # search for the best splitting value for the given column.\n","        gini_index, val = ## <-- EDIT THIS LINE\n","        if gini_index < min_gini_index:\n","            min_gini_index, split_column, split_val = gini_index, column, val\n","\n","    return min_gini_index, split_column, split_val"],"metadata":{"id":"a7JBa9PAAyZo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Since `build_tree` depends on `gini_split`, we need to slightly modify it to call `gini_split_rf` instead."],"metadata":{"id":"Naimaw_mBFrM"}},{"cell_type":"code","source":["# EDIT THIS FUNCTION\n","def build_tree_rf(n_features, X, y, columns_dict, feature_names, depth,  max_depth=10, min_samples_leaf=2):\n","    \"\"\"Build the decision tree according to the data.\n","    Args:\n","        X: (np.array) training features, of shape (N, p).\n","        y: (np.array) vector of training labels, of shape (N,).\n","        columns_dict: a dictionary mapping column indices to whether the column is categorical or numerical variable.\n","        feature_names (list): record the name of features in X in the original dataset.\n","        depth (int): current depth for this node.\n","    Returns:\n","        (dict): a dict denoting the decision tree (binary-tree). Each node has seven attributes:\n","          1. 'feature_name': The column name of the split.\n","          2. 'feature_index': The column index of the split.\n","          3. 'value': The value used for the split.\n","          4. 'number of data points': The number of data points in this node.\n","          5. 'Gini-index': The GINI-index of this node.\n","          6. 'categorical': indicator for categorical/numerical variables.\n","          7. 'class_probabilities': For leaf nodes, this stores ALL class probabilities. Otherwise, it is None.\n","          8. 'left': The left sub-tree with the same structure.\n","          9. 'right' The right sub-tree with the same structure.\n","    \"\"\"\n","    # include a clause for the cases where (i) all lables are the same, (ii) depth exceeded (iii) X is too small\n","    if len(np.unique(y)) == 1 or depth>max_depth or len(X)<=min_samples_leaf:\n","        return {'number of data points': len(y), 'class_probabilities': impurity(y)}\n","\n","    else:\n","        GI, split_column, split_val = ## <-- EDIT THIS LINE\n","\n","        # If GI is infinity, it means that samples are not separable by the sampled features.\n","        if GI == np.inf:\n","            return {'number of data points': len(y), 'class_probabilities': impurity(y)}\n","        categorical = columns_dict[split_column]\n","        (X_l, y_l), (X_r, y_r) = ## <-- EDIT THIS LINE\n","        return {\n","            'feature_name': feature_names[split_column],\n","            'feature_index': split_column,\n","            'value': split_val,\n","            'number of data points': len(y),\n","            'Gini-index': GI,\n","            'categorical': categorical,\n","            'class_probabilities': None,\n","            'left': build_tree_rf(n_features, X_l, y_l, columns_dict, feature_names, depth + 1, max_depth, min_samples_leaf),\n","            'right': build_tree_rf(n_features, X_r, y_r, columns_dict, feature_names, depth + 1, max_depth, min_samples_leaf)\n","        }"],"metadata":{"id":"-HMP1-Z_A7yK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a name=\"section-2c\"></a>\n","\n","## (2c) Training: Bootstrapping Implementation [^](#outline)"],"metadata":{"id":"Yc9C_hI6HhR_"}},{"cell_type":"markdown","source":["Now it is time to write the training function the constructs multiple decision trees, each operating on a subset of samples (with replacement)."],"metadata":{"id":"bjbNhpLwBbGJ"}},{"cell_type":"code","source":["# EDIT THIS FUNCTION\n","def train_rf(B, n_features, X, y,  columns_dict, max_depth=10):\n","    \"\"\"\n","    Resample B datasets from the training data and build a decision tree for each of them.\n","    Args:\n","        B: number of decision trees.\n","        X: (pd.Dataframe) training features, of shape (N, p). Each X[i] is a training sample.\n","        y: (pd.Series) vector of training labels, of shape (N,). y[i] is the label for X[i], and each y[i] is\n","        an integer in the range 0 <= y[i] <= C. Here C = 1.\n","        columns_dict: a dictionary mapping column indices to whether the column is categorical or numerical variable.\n","    \"\"\"\n","\n","    feature_names = X.columns.tolist()\n","    X = X.to_numpy()\n","    y = y.to_numpy()\n","    N = X.shape[0]\n","    training_indices = np.arange(N)\n","    trees = []\n","\n","    for _ in range(B):\n","        # Sample the training_indices (with replacement)\n","        sample = rng.choice() ## <-- EDIT THIS LINE\n","        X_sample = X[sample, :]\n","        y_sample = y[sample]\n","        tree = build_tree_rf(n_features, X_sample, y_sample,\n","                        columns_dict, feature_names, depth=1, max_depth=max_depth)\n","        trees.append(tree)\n","\n","    return trees"],"metadata":{"id":"RTz5V66SBnK3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a name=\"section-2d\"></a>\n","\n","## (2d) Classification: Aggregation [^](#outline)"],"metadata":{"id":"9GcTy26LIsrf"}},{"cell_type":"markdown","source":["As discussed in the lecture, we want to compute the aggregated, average probability vector over all Decision Trees. Hence, we first need to alter the `classify` function to output all class probabilities instead of just the majority vote."],"metadata":{"id":"SHfPWj2SpwKJ"}},{"cell_type":"code","source":["# EDIT THIS FUNCTION\n","def classify_rf(tree, x):\n","    \"\"\"\n","    Classify a single sample with the fitted decision tree.\n","    Args:\n","        x: ((pd.Dataframe) a single sample features, of shape (p,).\n","    Returns:\n","        (dict): class probabilities\n","    \"\"\"\n","    if tree['class_probabilities'] is not None:\n","        # return all class probabilities\n","        return tree['class_probabilities']\n","\n","    elif tree['categorical']:\n","        if x[tree['feature_index']] == tree['value']:\n","            # go to left branch\n","            return classify_rf(tree['left'], x)\n","        else:\n","            # go to right branch\n","            return classify_rf(tree['right'], x)\n","\n","    else:\n","\n","        if x[tree['feature_index']] ## <-- EDIT THIS LINE\n","            # go to left branch\n","            return ## <-- EDIT THIS LINE\n","        else:\n","            # go to right branch\n","            return ## <-- EDIT THIS LINE"],"metadata":{"id":"Ivom6xhCqdc-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's write the prediction function which aggregates the decision from all decision trees and returns the class with highest probability."],"metadata":{"id":"wCqwdq_uJ_6b"}},{"cell_type":"code","source":["# EDIT THIS FUNCTION\n","def aggregate(decisions):\n","    \"\"\"\n","    This function takes a list of predicted labels produced by a list\n","    of decision trees, aggregates the decisions and returns the class with highest probability from the aggregated decision.\n","    \"\"\"\n","    mean_class_probabilities = defaultdict(int)\n","\n","    for decision in decisions:\n","      for label,prob in decision.items():\n","        mean_class_probabilities[label] += prob\n","\n","    return ## <-- EDIT THIS LINE\n","\n","def predict_rf(rf, X):\n","    \"\"\"\n","    Predict classification results for X.\n","    Args:\n","        rf: A trained random forest through train_rf function.\n","        X: (pd.Dataframe) testing sample features, of shape (N, p).\n","    Returns:\n","        (np.array): predicted testing sample labels, of shape (N,).\n","    \"\"\"\n","\n","    if len(X.shape) == 1:\n","        # if we have one sample\n","        return aggregate([classify_rf(tree, X) for tree in rf])\n","    else:\n","        # if we have multiple samples\n","        return np.array([aggregate([classify_rf(tree, x) for tree in rf]) for x in X])"],"metadata":{"id":"TurQWLyZJ_L7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## EDIT THIS FUNCTION\n","def rf_score(rf, X_test, y_test):\n","    y_pred = ## <-- EDIT THIS LINE\n","    return np.mean(y_pred==y_test)"],"metadata":{"id":"mkYPt9KNLT5f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["n_features = int(np.sqrt(X_train.shape[1]))\n","B = 50\n","# fit the random forest with training data\n","rf = train_rf(B, n_features, X_train, y_train, columns_dict)"],"metadata":{"id":"4dBqmDLlJIQy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Training set accuracy: ',rf_score(rf, X_train.to_numpy(), y_train.to_numpy()))"],"metadata":{"id":"04z7XrNxLmEm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Test set accuracy: ',rf_score(rf, X_test.to_numpy(), y_test.to_numpy()))"],"metadata":{"id":"ta4lHvovRDGV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check you result (Warning: use only if you run the cells sequentially from the beginning)\n","rf_score(rf, X_train.to_numpy(), y_train.to_numpy())==1.0 and rf_score(rf, X_test.to_numpy(), y_test.to_numpy()) == 0.9555555555555556\n"],"metadata":{"id":"094tIZ1vRIoF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Questions on Random Forest:\n","\n","1. Compare the test accuracy of a sole decision tree with random forest. What do you conclude?\n","2. How to make Random Forest decisions ROC-curve-compatible (i.e. decisions with confidence scores). **Suggest** what can be used as score (or confidence) to be associated with each decision. Try to plot a diagram with two ROC-curves:\n","  1. for decision tree model.\n","  2. for random forest model.\n","3. Can you replicate your results using [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)?\n","4. How can we improve the interpretability of random forests?\n"],"metadata":{"id":"ojYUTxEDmy1u"}}]}