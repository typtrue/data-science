{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"dY98Oclcl92-"},"source":["# Naive Bayes\n","\n","Naive Bayes is a classification algorithm based on Bayes' theorem. Bayes’ theorem provides a way to calculate the probability of a data point belonging to a given class, given our prior knowledge. It is defined as\n","\n","$$\n","P (class|data) = \\frac{P (data|class) \\ P (class)}{P (data)} ,\n","$$\n","\n","where $P (class | data)$ is the probability over the potential classes given the provided data. The different probabilities $P$ you see in the equations above are commonly called prior, likelihood, evidence, and posterior as follows.\n","\n","$$\n","\\overbrace{P (class|data)}^{\\text{posterior}} = \\frac{\\overbrace{P (data|class)}^{\\text{likelihood}} \\ \\overbrace{P (class)}^{\\text{prior}}}{\\underbrace{P (data)}_{\\text{evidence}}}\n","$$\n","\n","The algorithm is called 'naive', because of its assumption that features of data are independent given the class label. Let us call the data features $x_1, \\dots, x_j, \\dots, x_p$ and the class label $y$, and rewrite Bayes theorem in these terms:\n","\n","$$\n","P (y|x_1, \\dots, x_p) = \\frac{P (x_1, \\dots, x_p|y) * P (y)}{P (x_1, \\dots, x_p)} \\, .\n","$$\n","\n","Then, the naive assumption of conditional independence between any two features given the class label can be expressed as\n","\n","$$\n","P(x_1, \\dots, x_p | y) = \\prod_{j=1}^p P(x_j | y) \\, ,\n","$$\n","\n","and now Bayes' theorem leads to:\n","\n","$$\n","P (y | x_1, \\dots, x_p) = \\frac{P (y) \\prod_{j=1}^p P(x_j | y)}{P (x_1, \\dots, x_p)} \\, .\\tag{1}\n","$$\n","\n","<!-- Since $P (x_1, \\dots, x_p)$ is the constant input, we can define the following proportional relationship\n","\n","$$\n","P (y|x_1, \\dots, x_p) \\propto P (y) \\prod_{i=1}^p P(x_i | y) \\, ,\n","$$\n","\n","and can use it to classify any data point as\n","\n","$$\n","\\hat y = \\underset{y}{\\text{arg max}} \\ P (y) \\prod_{i=1}^p P(x_i | y) \\, .\n","$$ -->\n","\n","We can use the posterior distribution to classify any data point as\n","\n","$$\n","\\hat y = \\underset{y}{\\text{arg max}} \\ P (y| x_1, \\dots, x_p)\\, .\\tag{2}\n","$$"]},{"cell_type":"markdown","source":["## Prepare data for text classification\n","\n","To learn how this algorithm works in practice, we define a simple data set of emails being either spam or not (adopted from Chapter 3.5, Exercise 3.22 in Machine Learning: A Probabilistic Perspective by Murphy). _Note that Naive Bayes can indeed be used for multiclass classification (see lecture notes), however we use it here as a binary classifier._\n","\n","We will work with the packages numpy and pandas."],"metadata":{"id":"p0UPRbx3QVN_"}},{"cell_type":"code","metadata":{"id":"8r_Xo0HklZFx"},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","from typing import Callable\n","\n","# Imports used for testing.\n","import numpy.testing as npt\n","\n","SMALL_SIZE = 12\n","MEDIUM_SIZE = 16\n","BIGGER_SIZE = 20\n","plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n","plt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the axes title\n","plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n","plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n","plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n","plt.rc('legend', fontsize=MEDIUM_SIZE)    # legend fontsize\n","plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Following Murphy, we create a toy dataset for spam email classification."],"metadata":{"id":"t4XxAWOYIdpn"}},{"cell_type":"code","source":["# define vocabulary\n","vocab = [\n","        'secret', 'offer', 'low', 'price', 'valued', 'customer', 'today',\n","        'dollar', 'million', 'sports', 'is', 'for', 'play', 'healthy', 'pizza'\n","    ]\n","\n","# define train spam emails\n","spam = [\n","    'million dollar offer',\n","    'secret offer today',\n","    'secret is secret'\n","]\n","\n","# define train non-spam emails\n","not_spam = [\n","    'low price for valued customer',\n","    'play secret sports today',\n","    'sports is healthy',\n","    'low price pizza'\n","]"],"metadata":{"id":"cq-2Xo5qIZzp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next we need to bring the toy data into a numerical form fit for applying machine learning models. We define the $N\\times p$ [*document-term matrix*](https://en.wikipedia.org/wiki/Document-term_matrix) $X$, which counts the frequency of $p$ different words in a corpus of $N$ documents. The document-term matrix is a standard data format in natural language processing (NLP) related to the so-called *bag-of-words model*. The rows of $X$ denoted $\\mathbf{x}^{(i)}, i=1,\\ldots,N$, correspond to the documents and are represented by vectors $\\mathbf{x}^{(i)}=(x^{(i)}_1,\\ldots,x^{(i)}_p)$ where $x^{(i)}_j$ encodes the frequency of word $j$ in document $i$. Moreover, $y^{(i)}$ is the binary target variable that encodes whether document $i$ is spam ($y^{(i)}=1$) or not ($y^{(i)}=0$)."],"metadata":{"id":"uUNsknX9IvVN"}},{"cell_type":"code","metadata":{"id":"wbwhd07rsZxv"},"source":["def prepare_spam_dataset(vocab, spam, not_spam, show_X=True):\n","    \"\"\" Prepare spam toy dataset for MultinomialNB implementation.\n","\n","    Parameters:\n","        vocab (list[str]): List of words considered for document-term matrix\n","        spam (list[str]): List of spam documents\n","        not_spam (list[str]): List of non-spam documents\n","        show_X (bool): Prints document-term matrix as table\n","\n","\n","    Returns:\n","        X (np.array): Document-term matrix\n","        y (np.array): Indicator of whether or not message is spam\n","    \"\"\"\n","\n","    # corpus consists of spam and non-spam documents\n","    corpus = spam + not_spam\n","\n","    # get size of corpus\n","    N = len(corpus)\n","\n","    # get size of vocabulary\n","    p = len(vocab)\n","\n","    # create dictionary for vocabulary indices\n","    vocab_dict = {vocab[j] : j for j in range(p)}\n","\n","    # compute document term matrix\n","    X = np.zeros((N,p))\n","\n","    # iterate through words\n","    for i, doc in enumerate(corpus):\n","        # split string into list of words\n","        doc_words = doc.split()\n","        # iterate through words and update document-term matrix\n","        for word in doc_words:\n","            if word in vocab:\n","                j ## <-- EDIT THIS LINE\n","                X[i,j] ## <-- EDIT THIS LINE\n","\n","    if show_X:\n","        display(pd.DataFrame(X,columns=vocab))\n","\n","    # encode class of each document\n","    y = [1] * len(spam) + [0] * len(not_spam)  # storing our labels in a list (1 means spam email, 0 means no spam email)\n","\n","    return X, y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LAF17qr2sv9G"},"source":["# define our variables and print document-term matrix X\n","X, y = prepare_spam_dataset(vocab, spam, not_spam, show_X=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["By looking at the document-term matrix $X$ one can already recognize some patterns in the relationship between words and spam. For example, the word secret appears three times in spam emails (the first three rows) but only one time in non-spam documents (the last four rows)."],"metadata":{"id":"w3S8-GM0MCFx"}},{"cell_type":"markdown","metadata":{"id":"KReuBEwe1hye"},"source":["## Likelihood in Multinomial Naive Bayes\n","\n","Next, we train the Naive Bayes classifier with a `mnb_fit()` function where we define the prior and likelihood.\n","\n","Recall from our lectures that the prior is the probability distribution incorporating our knowledge of the classes (here spam and not-spam) and it can be directly computed from $y$ using the equation for the prior from the lecture notes.\n","\n","Furthermore, we compute the likelihood of a word appearing in a document given its class (spam or not-spam) under the assumption of multinomiallly distributed data. In particular,  we estimate $P(x_j|y)$ as the relative frequency of term $j\\in{1,2,...,p}$ in the documents of class $y$ and we also apply *Laplace smoothing* to avoid zero probabilities. This leads to\n","\n","$$\n","P(x_j|y) = \\frac{1+\\sum_{i=1}^N x_j^{(i)}I(y^{(i)} = y )}{p + \\sum_{i=1}^N \\left(\\sum_{j=1}^p x_j^{(i)}\\right)I(y^{(i)} = y)}, \\tag{3}\n","$$\n","\n","which ensures that $P(x_1|y)+\\ldots+P(x_p|y)=1$. This version of the Naive Bayes classifier is also called [*Multinomial Naive Bayes*](https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Multinomial_naive_Bayes) (MNB).\n"]},{"cell_type":"code","metadata":{"id":"aq5Bwv4h1hZk"},"source":["# EDIT THIS FUNCTION\n","def mnb_fit(X, y):\n","    \"\"\" Use training data to fit Multinomial Naive Bayes classifier.\n","\n","    Parameters:\n","      X (np.array): Features\n","      y (np.array): Categorical target\n","\n","    Returns:\n","      prior (np.array): Prior distribution of classes\n","      lk_word (np.array): Likelihood of words (features) to appear given class\n","    \"\"\"\n","\n","    # check if size of document-term matrix and target match\n","    assert X.shape[0] == len(y), \"Size of document-term matrix and target does not match.\"\n","\n","    # get size of vocabulary\n","    p = X.shape[1]\n","\n","    # define prior\n","    prior =  ## <-- EDIT THIS LINE\n","\n","    # reorder X as a 2-dimensional array; each dimension contains data examples of only one of our two classes\n","    X_by_class = [X[y==c] for c in np.unique(y)]\n","\n","    # count words in each class\n","    word_counts = np.array([X_c.sum(axis=0) for X_c in X_by_class])\n","\n","    # define likelihood P(x|y) using Laplace smoothing, shape: (Nc, n)\n","    lk_word = ## <-- EDIT THIS LINE\n","\n","    return prior, lk_word"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Verification:** To verify your implementation of `mnb_fit()`, you should expect the following cell to execute without error messages."],"metadata":{"id":"kZj7S0szVt7I"}},{"cell_type":"code","source":["# The three lines shoud verify your implementation of mnb_fit()\n","# on a small example of two documents\n","prior_verify, lk_word_verify = mnb_fit(np.array([[0,1],[1,0]]), np.array([0,1]))\n","npt.assert_allclose(prior_verify, np.array([0.5,0.5]))\n","npt.assert_allclose(lk_word_verify, np.array([[1/3,2/3],[2/3,1/3]]))"],"metadata":{"id":"jBmaBiSHWj4c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can now compute the prior and likelihood for our document-term matrix defined above."],"metadata":{"id":"kOuezV58WXbF"}},{"cell_type":"code","source":["# call function and print prior and likelihood.\n","prior, lk_word = mnb_fit(X, y)\n","print('Prior:', prior)\n","print('\\n----------------')\n","print('Likelihood that word is typical for not_spam: \\n', lk_word[0])\n","print('\\n----------------')\n","print('Likelihood that word is typical for spam: \\n', lk_word[1])"],"metadata":{"id":"B0wif-RB82zs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Questions:\n","1. What is the meaning of the likelihood $P(x_j|y)$ in the context of our toy example?\n","2. Plot the likelihood $P(x_j|y)$ of each word $j$ given the different classes and explain where the difference comes from."],"metadata":{"id":"29-oXoDURy6V"}},{"cell_type":"markdown","metadata":{"id":"tPfkKXvk7oyC"},"source":["## Posterior in Multinomial Naive Bayes\n","\n","Now we can predict whether any given email is spam or not. To do so we compute the posterior probability $P(y|\\mathbf{x})$ that a document $\\mathbf{x}$ is part of class $y$, which, in the context of Multinomial Naive Bayes is given by:\n","\n","$$\n","P(y|x_1,\\ldots,x_p) \\propto P(y) ∏_{j=1}^p P(x_j|y)^{x_j}\\tag{4}\n","$$\n","\n","As multiplication of many small values can lead to significant rounding errors, it's advantagous to carry out this computation in log space:\n","\n","$$\n","\\log P(y|x_1,\\ldots,x_p) \\propto \\log P(y) + \\sum_{j=1}^p x_j \\, \\log P(x_j|y)\\tag{5}\n","$$\n","\n","Note that the log-posterior is linear. We now implement a function that calculates the log-posterior using linear algebra and then returns the normalized (!) posterior probabilities."]},{"cell_type":"code","source":["# EDIT THIS FUNCTION\n","def mnb_predict_proba(X, prior, lk_word):\n","    \"\"\" Compute posterior probabiliy of class with Multinomal Naive Bayes.\n","\n","    Params:\n","      X (np.array): Features\n","      prior (np.array): Prior distribution of classes\n","      lk_word (np.array): Likelihood of words (features) to appear given class\n","\n","    Returns:\n","      posteriors (np.array): Posterior distribution of documents\n","    \"\"\"\n","\n","    # compute log-posterior\n","    log_posterior = ## <-- EDIT THIS LINE\n","\n","    # normalize to get full posterior distribution\n","    normalize_term = ## <-- EDIT THIS LINE\n","    posteriors = ## <-- EDIT THIS LINE\n","\n","    return posteriors"],"metadata":{"id":"KhsW-W6RMyrE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Verification:** To verify your implementation of `prdict_proba()`, you should expect the following cell to execute without error messages."],"metadata":{"id":"nNVU8BkUY3h7"}},{"cell_type":"code","source":["# The two lines shoud verify your implementation of mnb_predict_proba()\n","posteriors_verify = mnb_predict_proba(np.array([[0,1],[1,0]]), prior_verify, lk_word_verify)\n","npt.assert_allclose(posteriors_verify,np.array([[2/3,1/3],[1/3,2/3]]))"],"metadata":{"id":"95Y6smehX0kI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can now compute the posterior probabilities for our training data."],"metadata":{"id":"vVQvpaDLN2p9"}},{"cell_type":"code","metadata":{"id":"_ViH8LlA1hNn"},"source":["# compute full posterior distribution\n","posteriors = mnb_predict_proba(X, prior, lk_word)\n","print(\"Posteriors:\\n\", posteriors)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AUJxJ9h6C4g_"},"source":["Finally, we can classify the documents in a binary fashion by asserting any data points $X$ to the class $y$ with the highest probability (called the *argmax choice*)."]},{"cell_type":"code","metadata":{"id":"36IsDfzH1eXN"},"source":["# EDIT THIS FUNCTION\n","def predict(X, prior, lk_word):\n","    \"\"\" Predict class with highest probability.\n","\n","    Params:\n","      X (np.array): Features\n","      prior (np.array): Prior distribution of classes\n","      lk_word (np.array): Likelihood of words (features) to appear given class\n","\n","    Returns:\n","      y_pred (np.array): Predicted target\n","    \"\"\"\n","\n","    # prediction given by argmax choice\n","    predicted_probabilities = ## <-- EDIT THIS LINE\n","    y_pred = ## <-- EDIT THIS LINE\n","\n","    return y_pred"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here, we take the same emails we used for training our Multinomial Naive Bayes classifier to also to evaluate it. Usually, the evaluation happens on unseen emails (test data) and it is your task below to define a small test dataset. What are the predicted classes and what is the accuracy of the classifier?"],"metadata":{"id":"x4skIf2h4szZ"}},{"cell_type":"code","metadata":{"id":"FroLvChb1hKg"},"source":["# predict targets for training data with Multinomial Naive Bayes\n","preds = ## <-- EDIT THIS LINE\n","print(\"Predicted classes: \", preds)\n","# compute accuracy\n","print(f'Train accuracy: {...}') ## <-- EDIT THIS LINE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JHJr2ZflEFGp"},"source":["#### Questions:\n","1. Explain why the use of a multinomial distribution for the likelihood is justified when applying Naive Bayes to the document-term matrix.\n","2. At the example of Multinomial Naive Bayes for document classification, explain why Naive Bayes is called naive.\n","Can you think of a way to model text data differently so that Naive Bayes better captures the semantics of a document?\n","\n"]},{"cell_type":"markdown","source":["#### Questions:\n","3. Define your own three short emails as a test set and evaluate our Naive Bayes classifier on it without re-training it on them. What do you observe?\n","4. What words have you included in emails of the test set that make them being classified as spam or not spam?"],"metadata":{"id":"DvW_Icm8MelB"}},{"cell_type":"markdown","source":["#### Questions:\n","5. Can you replicate your results using [sklearn](https://scikit-learn.org/stable/modules/naive_bayes.html)?"],"metadata":{"id":"OxdKZSyy9tSZ"}}]}