{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iFagXc4cBhu"
      },
      "source": [
        "#### Prerequisites\n",
        "\n",
        "- Basic familiarity with [Numpy](https://numpy.org/doc/stable/user/quickstart.html)\n",
        "- Basic familiarity with [Pyplot](https://matplotlib.org/stable/tutorials/introductory/pyplot.html)\n",
        "- Basic familiarity with [Pandas](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html)\n",
        "\n",
        "\n",
        "## Outline\n",
        "\n",
        "- [Section 1](#section-1): Dataset preparation\n",
        "- [Section 2](#section-2): Linear SVM formulation\n",
        "- [Section 3](#section-3): SVM optimisation using gradient descent\n",
        "- [Section 4](#section-4): SVM optimisation using mini-batch stochastic gradient descent\n",
        "- [Section 5](#section-5): *T*-fold cross-validation to choose hardness parameter\n",
        "- [Section 6](#section-6): Kernelised SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Arz0uQUpJ8nx"
      },
      "source": [
        "# Support Vector Machines (SVMs)\n",
        "In this notebook, we will explore Support Vector Machines (SVMs), focusing on both the linear and kernalised approaches for binary classification. We will use gradient descent and mini-batch stochastic gradient descent for the optimisation of the hinge loss.\n",
        "\n",
        "We will work with the [Breast Cancer Wisconsin (Diagnostic) Data Set](https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data), which you first need to download and then load in this notebook. If you faced difficulties downloading this data set from Kaggle, you should download the file directly from Blackboard. The data set contains various aspects of cell nuclei of breast screening images of patients with _(malignant)_ and without _(benign)_ breast cancer. Our goal is to build a classification model that can take these aspects of an unseen breast screening image, and classify it as either malignant or benign.\n",
        "\n",
        "<a name=\"section-1\"></a>\n",
        "\n",
        "## Section 1: Data preparation\n",
        "\n",
        "If you run this notebook locally on your machine, you will simply need to place the `csv` file in the same directory as this notebook.\n",
        "If you run this notebook on Google Colab, you will need to\n",
        "\n",
        "  `from google.colab import files`\n",
        "\n",
        "  `upload = files.upload()`\n",
        "\n",
        "and then upload it from your local downloads directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeRdgi8X2_kD"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#from google.colab import drive\n",
        "\n",
        "SMALL_SIZE = 12\n",
        "MEDIUM_SIZE = 16\n",
        "BIGGER_SIZE = 20\n",
        "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
        "plt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the axes title\n",
        "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
        "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
        "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
        "plt.rc('legend', fontsize=MEDIUM_SIZE)    # legend fontsize\n",
        "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9FsQfQ1J4MI"
      },
      "outputs": [],
      "source": [
        "# Load data, drop the extra column, and set 'id' as the index\n",
        "data = pd.read_csv('data.csv').iloc[:, :-1].set_index('id')\n",
        "\n",
        "# Display the last few rows of the processed DataFrame\n",
        "print(data.shape)\n",
        "data.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74Tw6xk53p20"
      },
      "source": [
        "We can see that our data set has 569 samples and 30 columns. The label is `diagnosis` (either **M: malignant** or **B: benign**). We convert the categorical labels into 1 for **M** and -1 for **B**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zbSOnY06rwL"
      },
      "outputs": [],
      "source": [
        "# Convert categorical labels to numbers\n",
        "diag_map = {'M': 1.0, 'B': -1.0}\n",
        "data['diagnosis'] = data['diagnosis'].map(diag_map)\n",
        "\n",
        "# Put labels and features in different DataFrames\n",
        "y = data.loc[:, 'diagnosis'] # loc is typically used for label indexing\n",
        "X = data.iloc[:, 1:] # iloc is used for integer indexing\n",
        "\n",
        "print(y.tail())\n",
        "X.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we split the dataset into training and test sets and standardise both. Importantly, the mean and standard deviation used for standardisation are always derived from the training set, as in a real-world scenario, the test set statistics would not be available.\n",
        "\n",
        "For datasets with features on different scales, it is essential to standardise the data beforehand. In SVMs, this could have an impact on the decision boundary."
      ],
      "metadata": {
        "id": "l9d3sHxNNfQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def standardise(X, X_train_=None):\n",
        "    \"\"\"\n",
        "    Standardise features.\n",
        "\n",
        "    Parameters:\n",
        "        X (np.array): Feature matrix.\n",
        "        X_train_ (np.array): An optional feature matrix to compute the statistics\n",
        "            from before applying it to X. If None, just use X to compute the statistics.\n",
        "\n",
        "    Returns:\n",
        "        X_std (np.array): Standardised feature matrix\n",
        "    \"\"\"\n",
        "    if X_train_ is None:\n",
        "        X_train_ = X\n",
        "\n",
        "    mu = np.mean(X_train_, axis=0, keepdims=True)\n",
        "    sigma = np.std(X_train_, axis=0, keepdims=True)\n",
        "    X_std = (X - mu) / sigma\n",
        "\n",
        "    return X_std"
      ],
      "metadata": {
        "id": "36Ek33Agis9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcw02XXM8EAe"
      },
      "outputs": [],
      "source": [
        "# Stacking data X and labels y into one matrix\n",
        "data_split = np.hstack((X, y.values.reshape(-1, 1)))\n",
        "\n",
        "# We split the dataset into training (70%) and test (30%)\n",
        "split_rate = 0.7\n",
        "np.random.shuffle(data_split)\n",
        "train, test = np.split(data_split, [int(split_rate * data_split.shape[0])])\n",
        "\n",
        "# We separate features and labels\n",
        "X_train = train[:, :-1]\n",
        "y_train = train[:, -1].astype(float)\n",
        "X_test = test[:, :-1]\n",
        "y_test = test[:, -1].astype(float)\n",
        "\n",
        "# We standardise both training and test sets\n",
        "X_train_std = ## <-- EDIT THIS LINE\n",
        "X_test_std = ## <-- EDIT THIS LINE\n",
        "\n",
        "# Insert 1 in every row for intercept b\n",
        "X_train_intercept = np.hstack((X_train_std, np.ones((len(X_train_std), 1))))\n",
        "X_test_intercept = np.hstack((X_test_std, np.ones((len(X_test_std), 1))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMo_v3TGCJBD",
        "tags": []
      },
      "source": [
        "<a name=\"section-2\"></a>\n",
        "\n",
        "## Section 2: Linear SVM formulation\n",
        "\n",
        "We start with defining the loss function\n",
        "\n",
        "$$\n",
        "L (\\mathbf{w}, b) = \\frac{1}{2} \\| \\mathbf{w} \\|^2 + \\lambda \\sum_{i=1}^N \\max \\bigg( 0, 1-y^{(i)} (\\mathbf{x}^{(i)} \\cdot \\mathbf{w} + b) \\bigg) \\,\n",
        "$$\n",
        "where $\\mathbf{w}$ is the vector of weights, $\\lambda$ the regularisation parameter, and $b$ the intercept which is included in our `X` as an additional column of $1$'s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8NCZ2Wj8D8m"
      },
      "outputs": [],
      "source": [
        "# EDIT THIS FUNCTION\n",
        "def compute_loss(w, X, y, regul_strength=1e5):\n",
        "    \"\"\"\n",
        "    Compute the loss function for linear SVM.\n",
        "\n",
        "    Parameters:\n",
        "        w (np.array): Vector of weights (incl. bias).\n",
        "        X (np.array): Input features.\n",
        "        y (np.array): Labels.\n",
        "        regul_strength (float): Regularisation parameter.\n",
        "\n",
        "    Returns:\n",
        "        loss (float): Loss function as given by the equation above.\n",
        "\n",
        "    \"\"\"\n",
        "    assert X.shape[0] == y.shape[0]\n",
        "    assert X.shape[1] == w.shape[0]\n",
        "\n",
        "    n = X.shape[0]\n",
        "    distances = ## <-- EDIT THIS LINE\n",
        "    distances[distances < 0] = 0  # equivalent to max(0, distance)\n",
        "    hinge = ## <-- EDIT THIS LINE\n",
        "\n",
        "    # calculate loss (the last term comes from the inclusion of b in the X matrix)\n",
        "    return 0.5 * np.dot(w, w) + regul_strength * hinge - 0.5 * w[-1] ** 2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"section-3\"></a>\n",
        "\n",
        "## Section 3: SVM optimisation using gradient descent"
      ],
      "metadata": {
        "id": "K-kauTLFTQEE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One way to optimise the loss is by using gradient descent algorithm. To this end, we need to first implement a function for the gradients of the loss with respect to $\\boldsymbol w$."
      ],
      "metadata": {
        "id": "RbhY4Ic0TWRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EDIT THIS FUNCTION\n",
        "def calculate_loss_gradient(w, X_train, y_train, regul_strength=1e6):\n",
        "    \"\"\"\n",
        "    Calculate gradient of the loss.\n",
        "\n",
        "    Parameters:\n",
        "        w (np.array): Vector of weights (incl. bias).\n",
        "        X_train (np.array): Train data.\n",
        "        y_train (np.array): Train labels.\n",
        "        regul_strength (float): Regularisation parameter.\n",
        "\n",
        "    Returns:\n",
        "        grad_w (float): Gradient of the loss with respect to w.\n",
        "\n",
        "    \"\"\"\n",
        "    assert X_train.shape[0] == y_train.shape[0]\n",
        "    assert X_train.shape[1] == w.shape[0]\n",
        "\n",
        "    distance = 1 - (y_train * (X_train @ w))\n",
        "    dw = np.zeros(len(w))\n",
        "\n",
        "    we = w.copy() # To prevent overwriting w\n",
        "    we[-1] = 0 # To prevent having b in its derivative when adding the weights in distance.\n",
        "\n",
        "    for ind, d in enumerate(distance):\n",
        "        if max(0, d) > 0:\n",
        "            dw += ## <-- EDIT THIS LINE\n",
        "\n",
        "    dw += ## <-- EDIT THIS LINE\n",
        "\n",
        "    return dw"
      ],
      "metadata": {
        "id": "g8ijBljHTRC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the hinge loss is not differentiable at zero. To address this, we explicitly set the gradient at that point to a specific value (zero). Formally, this involves using the subgradient, a generalisation of gradients to non-differentiable functions. For further details, refer to *Convex Optimisation* by Stephen Boyd, Chapter 6.\n",
        "\n",
        "The two functions described earlier are then used with the **gradient descent** algorithm to iteratively update the model weights using a given learning rate, $\\eta$. Additionally, a stopping criterion is implemented to terminate the training process when the loss function changes by less than a specified percentage.\n",
        "\n",
        "The updates of e.g. the weights $\\mathbf{w}$ during training are governed by the following rule:\n",
        "\n",
        "$$\n",
        "\\mathbf{w}_{t+1} = \\mathbf{w}_{t} - \\eta \\nabla_{\\mathbf{w}} L(\\mathbf{w}_t, b_t; S)\n",
        "$$\n",
        "\n",
        "where $\\nabla_{\\mathbf{w}} L(\\mathbf{w}_t, b_t; S)$ is the gradient of the loss function over the data $S$."
      ],
      "metadata": {
        "id": "S-v550DlTqzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EDIT THIS FUNCTION\n",
        "def gradient_descent(X, y, max_epochs=2000, stop_criterion=0.01, learning_rate=1e-5, regul_strength=1e6, print_outcome=False):\n",
        "    \"\"\"\n",
        "    Performs gradient descent to optimise weights.\n",
        "\n",
        "    Parameters:\n",
        "        X (np.array): The feature matrix.\n",
        "        y (np.array): The target labels.\n",
        "        max_epochs (int): Maximum number of epochs(iteration steps).\n",
        "        stop_criterion (float): Percentage change in loss function to determine convergence.\n",
        "        learning_rate (float): Learning rate for gradient updates.\n",
        "        regul_strength (float): Regularisation parameter\n",
        "        print_outcome (bool): Whether to print progress during training.\n",
        "\n",
        "    Returns:\n",
        "        weights (np.array): Optimised weights.\n",
        "        loss_history (list): History of loss.\n",
        "        epochs_list (list): List of epoch numbers corresponding to the loss values.\n",
        "\n",
        "    \"\"\"\n",
        "    # Initialise weights to zero\n",
        "    weights = np.zeros(X.shape[1])\n",
        "    nth = 0\n",
        "    prev_loss = np.inf  # Initialise starting loss as infinity\n",
        "    loss_history = []  # Track loss values\n",
        "    epochs_list = []  # Track epoch numbers\n",
        "\n",
        "    for epoch in range(1, max_epochs+1):\n",
        "        # Compute the gradient\n",
        "        gradient = ## <-- EDIT THIS LINE\n",
        "\n",
        "        # Update weights\n",
        "        weights = ## <-- EDIT THIS LINE\n",
        "\n",
        "        # Check for convergence at 2^nth epoch or last epoch\n",
        "        if epoch == 2**nth or epoch == max_epochs:\n",
        "            # Compute the current loss\n",
        "            loss = ## <-- EDIT THIS LINE\n",
        "            if print_outcome:\n",
        "                print(f'Epoch: {epoch}, Loss: {loss:.6f}')\n",
        "\n",
        "            # Check if the stop criterion is met\n",
        "            if: ## <-- EDIT THIS LINE\n",
        "                return weights, loss_history, epochs_list\n",
        "\n",
        "            # Update tracking variables\n",
        "            prev_loss = loss\n",
        "            epochs_list.append(epoch)\n",
        "            loss_history.append(loss)\n",
        "            nth += 1\n",
        "\n",
        "    return weights, loss_history, epochs_list\n"
      ],
      "metadata": {
        "id": "EavZ9CBeTsRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can take these functions and train a linear SVM with our training data."
      ],
      "metadata": {
        "id": "FDCvic1LUZf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "regul_strength = 10 # we use a reasonable value, but later in the notebook we will find the optimal one\n",
        "w, loss_history, epochs = gradient_descent(X_train_intercept, y_train, max_epochs=2000, stop_criterion=0.001, learning_rate=1e-3, regul_strength=regul_strength, print_outcome=True)\n",
        "print('Training finished.')"
      ],
      "metadata": {
        "id": "neoy2DOwUSWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We plot the loss against the number of epochs to check convergence of gradient descent."
      ],
      "metadata": {
        "id": "HMSe6_JPUlqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(epochs, loss_history)\n",
        "plt.title(f'Training loss for $\\\\lambda$={regul_strength}')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LS8fZgvxUchY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To evaluate the mean accuracy in both train and test set, we write a small function called `score`."
      ],
      "metadata": {
        "id": "hrKGF7JYUiMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## EDIT THIS FUNCTION\n",
        "def score(w, X, y):\n",
        "    \"\"\"\n",
        "    Computes the mean accuracy.\n",
        "\n",
        "    Parameters:\n",
        "        w (np.array): Vector of weights (incl. bias).\n",
        "        X (np.array): The feature matrix.\n",
        "        y (np.array): The target labels.\n",
        "\n",
        "    Returns:\n",
        "        mean_accuracy (np.array): Accuracy score.\n",
        "\n",
        "    \"\"\"\n",
        "    y_preds = np.sign(X @ w)\n",
        "    assert y_preds.shape == y.shape\n",
        "    mean_accuracy = ## <-- EDIT THIS LINE\n",
        "\n",
        "    return mean_accuracy\n",
        "\n",
        "print('Accuracy on training set: {}'.format(score(w, X_train_intercept, y_train)))\n",
        "print('Accuracy on test set: {}'.format(score(w, X_test_intercept, y_test)))"
      ],
      "metadata": {
        "id": "DhKNFlKOUm11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bm_NG7EfAYfr"
      },
      "source": [
        "<a name=\"section-4\"></a>\n",
        "\n",
        "## Section 4: SVM optimisation using mini-batch stochastic gradient descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5X3_B9V-I0mX"
      },
      "source": [
        "Though gradient descent can be a good option, in some cases the dataset is too large to perform gradient descent so instead we perform stochastic gradient descent.\n",
        "\n",
        "\n",
        "The updates of the weights during training are governed by the following rule:\n",
        "\n",
        "$$\n",
        "\\mathbf{w}_{t+1} = \\mathbf{w}_{t} - \\eta \\nabla_{\\mathbf{w}} L(\\mathbf{w}_t, b_t; S_m)\n",
        "$$\n",
        "\n",
        "where $\\nabla_{\\mathbf{w}} L(\\mathbf{w}_t, b_t; S_m)$ is the gradient of the loss function, this time computed over a randomly sampled mini-batch $S_m$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-S8N9C78D5R"
      },
      "outputs": [],
      "source": [
        "# EDIT THIS FUNCTION\n",
        "def mini_batch_gradient_descent(X, y, batch_size=32, max_epochs=2000, stop_criterion=0.01, learning_rate=1e-5, regul_strength=1e6, print_outcome=False):\n",
        "    \"\"\"\n",
        "    Performs mini-batch stochastic gradient descent to optimise weights.\n",
        "\n",
        "    Parameters:\n",
        "        X (np.array): The feature matrix.\n",
        "        y (np.array): The target labels.\n",
        "        batch_size (int): Size of each mini-batch.\n",
        "        max_epochs (int): Maximum number of epochs.\n",
        "        stop_criterion (float): Percentage change in loss function to determine convergence.\n",
        "        learning_rate (float): Learning rate for gradient updates.\n",
        "        regul_strength (float): Regularisation parameter\n",
        "        print_outcome (bool): Whether to print progress during training.\n",
        "\n",
        "    Returns:\n",
        "        weights (np.array): Optimised weights.\n",
        "        loss_history (list): History of loss.\n",
        "        epochs_list (list): List of epoch numbers corresponding to the loss values.\n",
        "\n",
        "    \"\"\"\n",
        "    # Initialise weights to zero\n",
        "    weights = np.zeros(X.shape[1])\n",
        "    nth = 0\n",
        "    prev_loss = np.inf  # Initialise starting loss as infinity\n",
        "    loss_history = []  # Track loss values\n",
        "    epochs_list = []  # Track epoch numbers\n",
        "\n",
        "    for epoch in range(1, max_epochs+1):\n",
        "        # Shuffle data to prevent repeating update cycles\n",
        "        indices = np.random.permutation(len(y))\n",
        "        X_shuffled, y_shuffled = ## <-- EDIT THIS LINE\n",
        "\n",
        "        # Create mini-batches\n",
        "        for start_idx in range(0, len(y), batch_size):\n",
        "            end_idx = start_idx + batch_size\n",
        "            X_batch = ## <-- EDIT THIS LINE\n",
        "            y_batch = ## <-- EDIT THIS LINE\n",
        "\n",
        "            # Compute the gradient for the mini-batch\n",
        "            gradient = ## <-- EDIT THIS LINE\n",
        "\n",
        "            # Update weights\n",
        "            weights = ## <-- EDIT THIS LINE\n",
        "\n",
        "        # Check for convergence at 2^nth epoch or last epoch\n",
        "        if epoch == 2**nth or epoch == max_epochs:\n",
        "            # Compute the current loss\n",
        "            loss = ## <-- EDIT THIS LINE\n",
        "            if print_outcome:\n",
        "                print(f'Epoch: {epoch}, Loss: {loss:.6f}')\n",
        "\n",
        "            # Check if the stop criterion is met\n",
        "            if: ## <-- EDIT THIS LINE\n",
        "                return weights, loss_history, epochs_list\n",
        "\n",
        "            # Update tracking variables\n",
        "            prev_loss = loss\n",
        "            epochs_list.append(epoch)\n",
        "            loss_history.append(loss)\n",
        "            nth += 1\n",
        "\n",
        "    return weights, loss_history, epochs_list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTU9fyVYK8ay"
      },
      "source": [
        "Now, we train the linear SVM with stochastic gradient descent instead. Note how the loss values have changed compared to gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bdsUvtu8D2f"
      },
      "outputs": [],
      "source": [
        "# Training the model\n",
        "regul_strength = 10 # we use a reasonable value, but later in the notebook we will find the optimal one\n",
        "w, loss_history, epochs = mini_batch_gradient_descent(X_train_intercept, y_train, max_epochs=2000, stop_criterion=0.001, learning_rate=1e-3, regul_strength=regul_strength, print_outcome=True)\n",
        "print('Training finished.')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, we can plot the loss against the epochs to check convergence.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "dCB0b-u22ZkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(epochs, loss_history)\n",
        "plt.title(f'Training loss for $\\\\lambda$={regul_strength}')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8gTUM66D2Y3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GQqts4_O1WE"
      },
      "source": [
        "And calculate the test and train accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JahERSXOtJj"
      },
      "outputs": [],
      "source": [
        "print('Accuracy on training set: {}'.format(score(w, X_train_intercept, y_train)))\n",
        "print('Accuracy on test set: {}'.format(score(w, X_test_intercept, y_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CePz4SbER-Qb"
      },
      "source": [
        "#### Questions:\n",
        "1. What are other evaluation metrics besides the accuracy? Implement them and assess the performance of our classification algorithm with them.\n",
        "2. What makes other evaluation metrics more appropriate given our unbalanced data set _(we have more benign than malignant samples)_?\n",
        "3. Try different learning rates, regularisation strengths and number of epochs independently. What can you observe? Can you achieve higher accuracies?\n",
        "4. Can you explain the rationale behind using hinge loss with this dataset containing 30 features?\n",
        "5. Can you comment on the differences in the results for gradient descent and stochastic gradient descent?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mz-IlM3mHGv"
      },
      "source": [
        "<a name=\"section-5\"></a>\n",
        "\n",
        "## Section 5: *T*-fold cross-validation to choose hardness parameter\n",
        "\n",
        "We now extend the procedure described above by applying it within a *T*-fold cross-validation framework, using multiple training-test splits instead of a single split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wk-Bov5K2we"
      },
      "outputs": [],
      "source": [
        "#EDIT THIS FUNCTION\n",
        "def cross_val_split(num_samples, num_folds):\n",
        "    \"\"\"\n",
        "    Splits the dataset indices into `num_folds` folds for cross-validation.\n",
        "\n",
        "    Parameters:\n",
        "        num_samples (int): The total number of samples in the dataset.\n",
        "        num_folds (int): The number of folds for cross-validation.\n",
        "\n",
        "    Returns:\n",
        "        fold (list[np.array]): A list of numpy arrays containing the indices defining the validation set for one fold.\n",
        "    \"\"\"\n",
        "\n",
        "    fold_size = num_samples // num_folds\n",
        "    shuffled_indices = np.random.permutation(np.arange(num_samples))\n",
        "    folds = ## <-- EDIT THIS LINE\n",
        "\n",
        "    return folds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yjf6Xa-QmK7s"
      },
      "outputs": [],
      "source": [
        "#EDIT THIS FUNCTION\n",
        "def cross_val_evaluate(data, num_folds, regul_strength_values):\n",
        "    \"\"\"\n",
        "    Performs T-fold cross-validation on the given dataset to find the optimal regul_strength.\n",
        "\n",
        "    Parameters:\n",
        "        data (np.array): The dataset, where the last column contains the target labels.\n",
        "        num_folds (int): The number of folds for cross-validation.\n",
        "        regul_strength_values (list): A list of regularisation strength values to evaluate.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing two lists:\n",
        "            - train_scores (list): Accuracy scores on the training set for each regul_strength.\n",
        "            - val_scores (list): Accuracy scores on the validation set for each regul_strength.\n",
        "    \"\"\"\n",
        "\n",
        "    folds = cross_val_split(data.shape[0], num_folds)\n",
        "    train_scores = np.zeros(len(regul_strength_values))\n",
        "    val_scores = np.zeros(len(regul_strength_values))\n",
        "\n",
        "    for j, regul_strength in enumerate(regul_strength_values):\n",
        "        print(f'\\nEvaluating regul_strength: {regul_strength}')\n",
        "\n",
        "        for i, val_indices in enumerate(folds):\n",
        "\n",
        "            # Define the training and validation sets\n",
        "            train_indices = np.setdiff1d(np.arange(data.shape[0]), val_indices)\n",
        "            X_train, y_train = data[train_indices, :-1], data[train_indices, -1]\n",
        "            X_val, y_val = data[val_indices, :-1], data[val_indices, -1]\n",
        "\n",
        "            # We standardise both training and validation sets\n",
        "            X_train_std = ## <-- EDIT THIS LINE\n",
        "            X_val_std = ## <-- EDIT THIS LINE\n",
        "\n",
        "            # Add intercept term (bias) to the feature matrices\n",
        "            X_train = np.hstack((X_train_std, np.ones((len(X_train_std), 1))))\n",
        "            X_val = np.hstack((X_val_std, np.ones((len(X_val_std), 1))))\n",
        "\n",
        "            # Train the model using mini-batch stochastic gradient descent\n",
        "            weights, _, _ = ## <-- EDIT THIS LINE\n",
        "\n",
        "            # Evaluate performance on validation set for the corresponding fold\n",
        "            train_scores[j] += ## <-- EDIT THIS LINE\n",
        "            val_scores[j] += ## <-- EDIT THIS LINE\n",
        "        print(f'Training score: {train_scores[j]}')\n",
        "        print(f'Validation score: {val_scores[j]}')\n",
        "\n",
        "    return train_scores, val_scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poufUfxFmK47"
      },
      "outputs": [],
      "source": [
        "regul_strength_values = [10**i for i in range(-3, 6)]\n",
        "train_scores, val_scores = cross_val_evaluate(train, 5, regul_strength_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also plot the training and validation accuracy for different values of the regularisation parameter. What does this show?"
      ],
      "metadata": {
        "id": "_nmPXOxyBbIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(-3, 6), train_scores, label='Training score')\n",
        "plt.plot(range(-3, 6), val_scores, label='Validation score')\n",
        "plt.title(fr'Mean accuracy for different $\\lambda$')\n",
        "plt.xlabel('Regularisation strength ($10^i$)')\n",
        "plt.ylabel('Mean accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hCV9mnxll7k1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovZTLlNRmJmC"
      },
      "outputs": [],
      "source": [
        "# We extract the optimal value of lambda\n",
        "regul_strength_opt = regul_strength_values[np.argmax(val_scores)]\n",
        "regul_strength_opt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0V8qyYIgmPsk"
      },
      "source": [
        "At this stage, we need to retrain the model using the optimal $\\lambda$ and then evaluate its performance on the original test set."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-training the model\n",
        "regul_strength = regul_strength_opt # optimal value\n",
        "w, loss_history, epochs = mini_batch_gradient_descent(X_train_intercept, y_train, max_epochs=2000, stop_criterion=0.001, learning_rate=1e-3, regul_strength=regul_strength, print_outcome=True)\n",
        "\n",
        "print('Training finished.')\n",
        "plt.plot(epochs, loss_history)\n",
        "plt.title(f'Training loss for $\\lambda$={regul_strength}')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n",
        "\n",
        "print('Accuracy on training set: {}'.format(score(w, X_train_intercept, y_train)))\n",
        "print('Accuracy on test set: {}'.format(score(w, X_test_intercept, y_test)))"
      ],
      "metadata": {
        "id": "M-uhSvsx0KT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wM-uuYh8k-aa"
      },
      "source": [
        "<a name=\"section-6\"></a>\n",
        "\n",
        "## Section 6: Kernelised SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_ScSxbBk-aa"
      },
      "source": [
        "In the following, we implement a soft-margin kernelised SVM classifier with a non-linear kernel. Here, we use a Gaussian kernel:\n",
        "$$k(x,y|\\sigma) = e^{-\\frac{||x-y||^2}{\\sigma}}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGgr4vsmk-aa"
      },
      "outputs": [],
      "source": [
        "# EDIT THIS FUNCTION\n",
        "def kernel_matrix(X1, X2, sigma):\n",
        "    \"\"\"\n",
        "    Calculates the kernel matrix given the data.\n",
        "\n",
        "    Parameters:\n",
        "        X1, X2 (np.array): Two sets of input features.\n",
        "        sigma (float): Hyperparameter in Gaussian kernel.\n",
        "\n",
        "    Returns:\n",
        "        kernel (np.array): Kernel Matrix.\n",
        "    \"\"\"\n",
        "    n1, m1 = X1.shape\n",
        "    n2, m2 = X2.shape\n",
        "    kernel = np.zeros((n1, n2))\n",
        "\n",
        "    # Here we define a Gaussian Radial Basis Function Kernel\n",
        "    for i in range(n1):\n",
        "        exponent = ## <-- EDIT THIS LINE\n",
        "        kernel[i,:] = np.exp(-exponent / sigma)\n",
        "\n",
        "    return kernel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSl4CBUhk-ab"
      },
      "source": [
        "As we have seen in the lecture notes, the kernel trick can be implemented in the primal-form problem by defining a new loss function:\n",
        "\n",
        "$$L(\\mathbf{u},b) = \\frac{1}{2}\\mathbf{u}^{\\rm{T}}\\mathbf{K} \\mathbf{u} + \\lambda \\sum_{i=1}^N  \\max \\Big\\{0, 1-y^{(i)}(\\mathbf{K}^{(i)}\\mathbf{u} + b)\\Big\\},$$\n",
        "\n",
        "where $\\mathbf{K}$ is the Gram matrix containing the kernel functions, i.e.\n",
        " $\\mathbf{K}_{ij} = k(\\mathbf{x}^{(i)},\\mathbf{x}^{(j)})$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTimbD48k-aa"
      },
      "source": [
        "Having defined the kernel, we use this to compute the loss below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztKFVrs7k-aa"
      },
      "outputs": [],
      "source": [
        "## EDIT THIS FUNCTION\n",
        "def compute_loss_kernel(u, K, y, regul_strength=1e3, intercept=0):\n",
        "    \"\"\"\n",
        "    Computes the loss using the hinge loss with the kernel trick.\n",
        "\n",
        "    Parameters:\n",
        "        u (np.array): Weight vector in the kernel space.\n",
        "        K (np.array): Kernel matrix.\n",
        "        y (np.array): Target labels (binary, e.g., -1 and 1).\n",
        "        regul_strength (float): Regularisation strength (lambda).\n",
        "        intercept (float): Intercept term.\n",
        "\n",
        "    Returns:\n",
        "        total_loss (float): Computed loss value.\n",
        "    \"\"\"\n",
        "    # Here we define the hinge loss with the kernel trick. NB: the intercept should be kept separate #\n",
        "    distances = ## <-- EDIT THIS LINE\n",
        "    distances[distances < 0] = 0  # equivalent to max(0, distance)\n",
        "    hinge = ## <-- EDIT THIS LINE\n",
        "\n",
        "    # Calculate loss\n",
        "    return ## <-- EDIT THIS LINE"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To perform the optimisation, we simply modify the functions introduced above.  In previous parts of this notebook we included a vector of ones within $X_{\\mathrm{train}}$ and $X_{\\mathrm{test}}$  to learn the intercept term $b$, but within the kernelised formulation, one cannot readily employ this trick. For implementation of the kernelised SVM we will therefore use $X_{\\mathrm{train}}$ and $X_{\\mathrm{test}}$ **without** the additional vector of ones. The inclusion of an intercept term now needs to be explicitly considered when defining the loss function.\n",
        "\n",
        "Following the steps above, we want to optimise the loss is by using Gradient Descent (GD)."
      ],
      "metadata": {
        "id": "QJMXLSIGLEGh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYK58ipik-ab"
      },
      "outputs": [],
      "source": [
        "#EDIT THIS FUNCTION\n",
        "def calculate_loss_gradient_kernel(u, K_train, y_train, regul_strength=1e3, intercept=0):\n",
        "    \"\"\"\n",
        "    Computes the gradient of the loss with respect to u and intercept (b).\n",
        "\n",
        "    Parameters:\n",
        "        u (np.array): Weight vector in the kernel space.\n",
        "        K_train (np.array): Kernel matrix training set.\n",
        "        y_train (np.array): Target labels for the training set (binary, e.g., -1 and 1).\n",
        "        regul_strength (float): Regularisation strength (lambda).\n",
        "        intercept (float): Intercept term.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Gradients with respect to `u` (dw) and intercept (db).\n",
        "    \"\"\"\n",
        "    # If only one example is passed\n",
        "    if type(y_train) == np.float64 or type(y_train) == np.int32:\n",
        "        y_train = np.asarray([y_train])\n",
        "        K_train = np.asarray([K_train])  # gives multidimensional array\n",
        "\n",
        "\n",
        "    distance = ## <-- EDIT THIS LINE\n",
        "\n",
        "    dw = np.zeros(len(u))\n",
        "    db = 0\n",
        "\n",
        "    # Define the gradient with the hinge loss\n",
        "    for ind, d in enumerate(distance):\n",
        "        if d > 0:\n",
        "            dw += ## <-- EDIT THIS LINE\n",
        "            db += ## <-- EDIT THIS LINE\n",
        "\n",
        "    dw += K_train @ u\n",
        "\n",
        "    return dw , db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hecmIRH7k-af"
      },
      "outputs": [],
      "source": [
        "# EDIT THIS FUNCTION\n",
        "def gradient_descent_kernel(K, y, max_epochs=2000, stop_criterion=0.01, learning_rate=1e-5, regul_strength=1e6, print_outcome=False):\n",
        "    \"\"\"\n",
        "    Performs mini-batch stochastic gradient descent to optimise u and intercept.\n",
        "\n",
        "    Parameters:\n",
        "        X (np.array): The feature matrix.\n",
        "        y (np.array): The target labels.\n",
        "        max_epochs (int): Maximum number of epoch.\n",
        "        stop_criterion (float): Percentage change in loss function to determine convergence.\n",
        "        learning_rate (float): Learning rate for gradient updates.\n",
        "        regul_strength (float): Regularisation parameter\n",
        "        print_outcome (bool): Whether to print progress during training.\n",
        "\n",
        "    Returns:\n",
        "        u (np.array): Optimised weights.\n",
        "        intercept (np.array): Optimised intercept term\n",
        "        loss_history (list): History of loss.\n",
        "        epoch_list (list): List of epochs numbers corresponding to the loss values.\n",
        "\n",
        "    \"\"\"\n",
        "    # Initialise weights to zero\n",
        "    u = np.zeros(K.shape[0])\n",
        "    intercept = 0.0\n",
        "    nth = 0\n",
        "    prev_loss = np.inf  # Initialise starting loss as infinity\n",
        "    loss_history = []  # Track loss values\n",
        "    epoch_list = []  # Track epoch numbers\n",
        "\n",
        "\n",
        "    for epoch in range(1, max_epochs+1):\n",
        "        # Shuffle data to prevent repeating update cycles\n",
        "        indices = np.random.permutation(len(y))\n",
        "\n",
        "        # Calculate the gradient and update u and the intercept\n",
        "        ascent, ascent_intercept = ## <-- EDIT THIS LINE\n",
        "        u = ## <-- EDIT THIS LINE\n",
        "        intercept = ## <-- EDIT THIS LINE\n",
        "\n",
        "        # Check for convergence at 2^nth epoch or last epoch\n",
        "        if epoch == 2**nth or epoch == max_epochs:\n",
        "\n",
        "            # Compute the current loss\n",
        "            loss = ## <-- EDIT THIS LINE\n",
        "            if print_outcome:\n",
        "                print(f'Epoch: {epoch}, Loss: {loss:.6f}')\n",
        "\n",
        "            # Check if the stop criterion is met\n",
        "            if : ## <-- EDIT THIS LINE\n",
        "                return u, intercept\n",
        "\n",
        "            # Update tracking variables\n",
        "            prev_loss = loss\n",
        "            epoch_list.append(epoch)\n",
        "            loss_history.append(loss)\n",
        "            nth += 1\n",
        "\n",
        "    return u, intercept, loss_history, epoch_list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQnvsITFk-af"
      },
      "source": [
        "Finally, let's compare the performance for some different values of $\\sigma$."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def score_kernel(u, X, y, sigma, intercept):\n",
        "        \"\"\"\n",
        "        Computes the accuracy score for the given data using a kernelised model.\n",
        "\n",
        "        Parameters:\n",
        "            u (np.ndarray): Weight vector in the kernel space.\n",
        "            X (np.ndarray): Feature matrix for the data to be scored.\n",
        "            y (np.ndarray): True labels for the data.\n",
        "            sigma (float): Hyperparameter for the Gaussian kernel.\n",
        "            intercept (float): Intercept term.\n",
        "\n",
        "        Returns:\n",
        "            accuracy (float): Accuracy score, calculated as the proportion of correctly predicted labels.\n",
        "        \"\"\"\n",
        "        K_test = kernel_matrix(X, X_train, sigma)\n",
        "\n",
        "\n",
        "        y_preds = ## <-- EDIT THIS LINE\n",
        "        accuracy = np.mean(y_preds == y)\n",
        "\n",
        "        return accuracy"
      ],
      "metadata": {
        "id": "288MVVGtWRuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sigma in [10**i for i in range(0, 6)]:\n",
        "\n",
        "    print('For sigma = ' + str(sigma))\n",
        "    K_train = kernel_matrix(X_train, X_train, sigma) # Note that X_train does not have 1's attached\n",
        "\n",
        "    u,b = gradient_descent_kernel(K_train, y_train, max_epochs=2000, stop_criterion=0.001, learning_rate=1e-5, regul_strength=1e3, print_outcome=False)\n",
        "\n",
        "    print('Accuracy on training set: {}'.format(score_kernel(u, X_train, y_train, sigma, b)))\n",
        "    print('Accuracy on test set: {}'.format(score_kernel(u, X_test, y_test, sigma, b)))\n",
        "    print('Intercept: {}'.format(b))"
      ],
      "metadata": {
        "id": "-cirhIhmOIXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsNeAt6lk-af"
      },
      "source": [
        "#### Questions:\n",
        "1. What do you observe when $\\sigma$ varies?\n",
        "2. How does the Gaussian Kernel SVM compare to linear SVM?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}