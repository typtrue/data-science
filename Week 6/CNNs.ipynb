{"cells":[{"cell_type":"markdown","metadata":{"id":"NNqIbKuXY0OV"},"source":["# Convolutional Neural Networks (CNNs)\n","\n","The purpose of this notebook is to practice implementing and training CNNs. We start with a 1-dimensional convolutional layer in [NumPy](https://numpy.org/doc/).\n","\n","We will then use [PyTorch](https://pytorch.org/), an optimised machine learning framework for Python based on the `torch` library to implement machine learning architectures."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ii9JKVDtY0On"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import torch\n","\n","# Importing losses, activation functions and layers from PyTorch\n","from torch.nn import Sequential, CrossEntropyLoss, Conv1d, MaxPool1d, Flatten, Linear, ReLU, Softmax, Parameter\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","SMALL_SIZE = 12\n","MEDIUM_SIZE = 16\n","BIGGER_SIZE = 20\n","plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n","plt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the axes title\n","plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n","plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n","plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n","plt.rc('legend', fontsize=MEDIUM_SIZE)    # legend fontsize\n","plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title"]},{"cell_type":"markdown","metadata":{"id":"a3RNg0LRY0PH"},"source":["You will be working with the [Human Activity Recognition (HAR) Using Smartphones](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones) dataset. This dataset captures movement data from a smartphone's built-in accelerometer and gyroscope while a person performs different activities. The six recorded activities are: walking on flat ground, walking upstairs, walking downstairs, sitting, standing, and lying down.\n","\n","The smartphone records six measurements every 0.02 seconds: both linear acceleration and gyroscopic acceleration in the x, y, and z directions. Linear acceleration measures changes in speed without considering rotation, while gyroscopic acceleration tracks changes in orientation and angular velocity.\n","\n","The goal is to use the accelerometer data to **predict the type of activity** (a multi-class classification task)."]},{"cell_type":"markdown","metadata":{"id":"tvofb4QGXxlq"},"source":["## 1. Loading and plotting the data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e5qjIoByY0PI"},"outputs":[],"source":["x_train = np.load('x_train.npy')\n","y_train = np.load('y_train.npy')\n","\n","x_val = np.load('x_val.npy')\n","y_val = np.load('y_val.npy')\n","\n","x_test = np.load('x_test.npy')\n","y_test = np.load('y_test.npy')"]},{"cell_type":"markdown","metadata":{"id":"u_7_7QmLY0PJ"},"source":["The input data consists of 6 features over 128 timesteps, and the output, which we are trying to predict, is a single integer from 0 to 5, which denotes the class.\n","\n","There are 7352 examples in the training set, 2447 examples in the validation set and 500 in the test set. We can check the shapes ourselves."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kq1ijauTY0PK"},"outputs":[],"source":["print(x_train.shape)\n","print(y_train.shape)\n","print(x_val.shape)\n","print(y_val.shape)\n","print(x_test.shape)\n","print(y_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7aOvZYv6Xxlv"},"outputs":[],"source":["classes = [\n","    'Walking on flat ground',\n","    'Walking upstairs',\n","    'Walking downstairs',\n","    'Sitting',\n","    'Standing',\n","    'Laying'\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HlDpeL3WY0PL"},"outputs":[],"source":["# Plot a randomly selected example from each class\n","\n","for l, label in enumerate(range(len(classes))):\n","    inx = np.where(y_train[:, 0] == label)[0]\n","    i = np.random.choice(inx)\n","    x_example = x_train[i].T\n","    fig, ax = plt.subplots(figsize=(10, 1))\n","    ax.imshow(x_example.T, cmap='Greys', vmin=-1, vmax=1)\n","    ax.set_ylabel('Reading')\n","    ax.set_xlabel('Timestep')\n","    ax.set_title(classes[l])"]},{"cell_type":"markdown","metadata":{"id":"lsQNWPAiXxly"},"source":["## 2. 1D convolutional layer in NumPy"]},{"cell_type":"markdown","metadata":{"id":"6yXkPtG6Y0PL"},"source":["Now we will implement a 1D convolutional layer in `numpy`. The following function is designed to perform a 1D convolution on an input signal, given `weight` and `bias` parameters. The layer should have no `padding` and a `stride` of 1. The output should consist of the pre-activations of the layer, meaning that no activation function is then applied."]},{"cell_type":"markdown","metadata":{"id":"r4Xq_OW0Xxl0"},"source":["### Notation\n","\n","* `c_in` and `c_out` (equal to `n_filters`) represent the number of input and output channels (respectively).\n","* `i` and `o` denote the length of the input and output signals (respectively).\n","* `k` is the length of the convolving kernel/filter.\n","* `p` is the total number of zero-padding pixels added to the input (padding).\n","* `s` is the number of pixels the kernel shifts per step (stride).\n","\n","For a convolutional (or max pool) layer, the input and output lengths are related as follows:\n","\n","$$o=\\left\\lfloor\\frac{i+p-k}{s}\\right\\rfloor +1$$\n","(Note the use of the floor symbol, denoted as $\\lfloor x\\rfloor $, which represents the greatest integer less than or equal to a real number $x$)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uk33QUuQY0PM"},"outputs":[],"source":["## EDIT THIS FUNCTION\n","def conv1d(x, weight, bias):\n","    \"\"\"\n","    Performs a 1D convolution over an input signal.\n","\n","    Parameters:\n","        x (np.ndarray): Input signal of shape (batch_size, c_in, l_in)\n","        weight (np.ndarray): Learnable weights, shape (c_out, c_in, kernel_size)\n","        bias (np.ndarray): Bias parameters of size c_out\n","\n","    Returns:\n","        An array of shape (batch_size, c_out, l_out)\n","\n","    \"\"\"\n","    batch_size = ... ## <-- EDIT THIS LINE\n","    l_in = ... ## <-- EDIT THIS LINE\n","    c_out = ... ## <-- EDIT THIS LINE\n","    kernel_size = ... ## <-- EDIT THIS LINE\n","\n","    l_out = ... ## <-- EDIT THIS LINE\n","    outputs = np.zeros((batch_size, c_out, l_out))\n","    np.testing.assert_allclose(l_out, 113)\n","\n","    for i in range(l_out):\n","        outputs[:, :, i] = ... ## <-- EDIT THIS LINE\n","    return outputs"]},{"cell_type":"markdown","metadata":{"id":"m2nYllZWXxl1"},"source":["Now we can compare our layer with the PyTorch implementation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dshIXABmY0PN","scrolled":true},"outputs":[],"source":["# Test your layer\n","n_filters = 8\n","batch_size = 16\n","k = 16\n","l_in = 128\n","\n","# PyTorch\n","conv_layer = Conv1d(x_train.shape[1], n_filters, k)\n","inputs = torch.randn((batch_size, x_train.shape[1], l_in))\n","y_torch = conv_layer(inputs)\n","\n","# Our layer\n","y = conv1d(inputs.numpy(), conv_layer.weight.detach().numpy(), conv_layer.bias.detach().numpy())\n","np.allclose(y, y_torch.detach().numpy(), atol=1e-4)  ## <-- should be 'True'\n"]},{"cell_type":"markdown","metadata":{"id":"tMTX23iKXxl2"},"source":["### 3. Building a CNN in PyTorch"]},{"cell_type":"markdown","metadata":{"id":"lFoo0QAmY0PN"},"source":["Before we can train a CNN model, we first need to build one using PyTorch. We will utilise the `Sequential` class to define the architecture and set up the necessary hyperparameters. The model will be designed to process the HAR dataset and will include the following layers:\n","\n","* A `Conv1d` layer with 8 filters, kernel size of 16 and a `ReLU` activation function\n","  * The input shape should be `(6, 128)`\n","* A `MaxPoo1d` layer with a pooling window size of 16 and a stride of 2\n","* A `Flatten` layer\n","* A `Linear` layer with 6 neurons\n","\n","The function below should build and compile this model, using an `Adam` optimiser and a `CrossEntropyLoss` criterion."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xpMUSAAAXxl3"},"outputs":[],"source":["def get_model(x_train, n_filters, k, pool_size, stride_pool, classes):\n","    \"\"\"\n","    CNN model in PyTorch:\n","    - Layers are Conv1d(+ReLU), MaxPool1d, Flatten and Linear(+Softmax).\n","    - It features an Adam optimiser and CrossEntropyLoss criterion.\n","\n","    Parameters:\n","        x_train (np.ndarray): Training data\n","        n_filters (int): Number of filters to be used in the convolutional layer\n","        k (int): Kernel size in the convolutional layer\n","        pool_size (int): MaxPool1d window size\n","        stride_pool (int): Stride of the MaxPool1d sliding window\n","        classes (List): Output classes\n","\n","    Returns:\n","        Model, criterion and optimiser.\n","\n","    \"\"\"\n","\n","    l_out_conv = ... # Length after Conv1d (note that the stride is 1) ## <-- EDIT THIS LINE\n","    l_out_pool = ... # Length after MaxPool1d ## <-- EDIT THIS LINE\n","    l_in_linear = ... # Size before Linear layer ## <-- EDIT THIS LINE\n","    np.testing.assert_allclose(l_in_linear, 392)\n","\n","    model = Sequential(\n","        Conv1d(x_train.shape[1], n_filters, kernel_size=k),\n","        ..., ## <-- EDIT THIS LINE\n","        ..., ## <-- EDIT THIS LINE\n","        ..., ## <-- EDIT THIS LINE\n","        ..., ## <-- EDIT THIS LINE\n","    )\n","\n","    criterion = ... ## <-- EDIT THIS LINE\n","    optimiser = ... ## <-- EDIT THIS LINE\n","\n","    return model, criterion, optimiser\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cDMUuuScY0PP"},"outputs":[],"source":["# Run your function to get the model and print it\n","\n","n_filters = 8\n","k = 16\n","pool_size = 16\n","stride_pool = 2\n","\n","model, criterion, optimiser = ... ## <-- EDIT THIS LINE\n","print(model)"]},{"cell_type":"markdown","metadata":{"id":"ePI3PrR7Xxl4"},"source":["## 4. Training a CNN"]},{"cell_type":"markdown","metadata":{"id":"411MNlZxY0PP"},"source":["Before training, we need to prepare our data. Since Deep Learning models in PyTorch operate on tensors, we first convert our NumPy arrays into tensors. These are then wrapped into `TensorDataset` objects, which help structure the data for training and validation. To efficiently load and iterate through batches of data, we use PyTorch's `DataLoader`.\n","\n","Now we are ready to train the model. The `training_loop` function will handle the training process for a maximum of 200 epochs, with a batch size of 128. To ensure the model does not overfit and to prevent unnecessary training, we will implement early stopping based on validation accuracy. Specifically, the training will stop if the validation loss does not improve for 8 consecutive epochs (`max_patience=8`)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JgnrxG-BXxl5"},"outputs":[],"source":["# Numpy arrays to PyTorch tensors\n","x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n","y_train_tensor = torch.tensor(y_train)\n","x_val_tensor = torch.tensor(x_val, dtype=torch.float32)\n","y_val_tensor = torch.tensor(y_val)\n","\n","# Creating training and validation datasets\n","train_dataset = TensorDataset(x_train_tensor, y_train_tensor.squeeze())\n","val_dataset = TensorDataset(x_val_tensor, y_val_tensor.squeeze())\n","\n","# Creating corresponding DataLoaders\n","train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rVxttyIPXxl5"},"outputs":[],"source":["class EarlyStopping:\n","    \"\"\"\n","    EarlyStopping class.\n","\n","    Attributes:\n","        max_patience (int): Amount of epochs with no improvement after which training is stopped.\n","        patience (int): Stores the number of epochs with no improvement.\n","        best_valid_loss (float): Stores the current value of the best (minimum) validation loss.\n","        early_stop (bool) True if training needs to be stopped due to the early stopping condition being met.\n","\n","    Methods:\n","        step(val_loss):\n","            Checks current state after an epoch and updates best_loss, patience and early_stop accordingly.\n","    \"\"\"\n","\n","    def __init__(self, max_patience=5):\n","        self.max_patience = max_patience\n","        self.patience = 0\n","        self.best_valid_loss = ... ## <-- EDIT THIS LINE\n","        self.early_stop = False\n","\n","    def step(self, val_loss):\n","        if ... ## <-- EDIT THIS LINE\n","            self.best_valid_loss = ... ## <-- EDIT THIS LINE\n","            self.patience = ... ## <-- EDIT THIS LINE\n","        else:\n","            self.patience += 1\n","            if self.patience >= self.max_patience:\n","                self.early_stop = True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kHMHC68RXxl6"},"outputs":[],"source":["def training_loop(train_loader, val_loader, max_num_epochs=200, max_patience=5):\n","    \"\"\"\n","    Training loop with early stopping to monitor the validation accuracy.\n","\n","    Parameters:\n","        train_loader (torch.utils.data.DataLoader): Training DataLoader\n","        val_loader (torch.utils.data.DataLoader): Validation DataLoader\n","        max_num_epochs (int): Maximum number of epochs\n","        max_patience (int): max_patience attribute of the EarlyStopping class\n","\n","    Returns:\n","        Model, criterion and optimiser.\n","\n","    \"\"\"\n","\n","    history = {'training_loss': [], 'validation_loss': [], 'training_accuracy': [], 'validation_accuracy': []}\n","    early_stopping = EarlyStopping(max_patience=max_patience)\n","\n","    for epoch in range(max_num_epochs):\n","\n","        tr_loss = 0.0\n","        tr_correct = 0\n","        val_loss = 0.0\n","        val_correct = 0\n","\n","        # Training\n","        model.train()\n","        for inputs, labels in train_loader:\n","            optimiser.zero_grad() # Setting gradients to zero\n","            outputs = model(inputs)\n","            loss = ... ## <-- EDIT THIS LINE\n","            tr_loss += loss.item()\n","            tr_correct += ... ## <-- EDIT THIS LINE\n","            loss.backward() # Computes gradients of the loss\n","            optimiser.step() # Optimisation step (parameters are updated)\n","\n","        history['training_loss'].append(tr_loss/len(train_loader))\n","        history['training_accuracy'].append(100*tr_correct/len(train_dataset))\n","\n","        # Validation\n","        model.eval()\n","        with torch.no_grad():\n","            for inputs_v, labels_v in val_loader:\n","                outputs_v = model(inputs_v)\n","                loss_v = ... ## <-- EDIT THIS LINE\n","                val_loss += loss_v.item()\n","                if labels_v.size(0):\n","                    val_correct += ... ## <-- EDIT THIS LINE\n","\n","        history['validation_loss'].append(val_loss/len(val_loader))\n","        history['validation_accuracy'].append(100*val_correct/len(val_dataset))\n","\n","        # Print progress every 10 epochs\n","        if (epoch + 1) % 10 == 0 or epoch == 0:\n","            print(f'Epoch {epoch + 1}/{max_num_epochs}, Training loss: {tr_loss/len(train_loader)}, Training accuracy: {100*tr_correct/len(train_dataset)}%, Validation loss: {val_loss/len(val_loader)}, Validation accuracy: {100*val_correct/len(val_dataset)}%')\n","\n","        # Check for early stopping\n","        early_stopping.step(val_loss / len(val_loader))\n","        if early_stopping.early_stop:\n","            print('...')  ## <-- EDIT THIS LINE\n","            ... ## <-- EDIT THIS LINE\n","\n","    return history\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q0GM1QxfNqVD"},"outputs":[],"source":["# Calling the training loop\n","max_num_epochs = 200\n","max_patience = 8\n","\n","history = training_loop(train_loader, val_loader, max_num_epochs, max_patience)"]},{"cell_type":"markdown","metadata":{"id":"hGsROXUPoiBo"},"source":["We visualise loss and accuracy over epochs to assess the model's learning progress, helping us evaluate its convergence and generalisation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YC7k96O0Y0PQ"},"outputs":[],"source":["# Plot the learning curves\n","\n","fig = plt.figure(figsize=(12, 5))\n","\n","fig.add_subplot(121)\n","plt.plot(history['training_loss'], label='Train')\n","plt.plot(history['validation_loss'], label='Validation')\n","plt.xlabel('Epoch', size=12)\n","plt.ylabel('Cross-entropy loss', size=12)\n","plt.title('Learning curve')\n","plt.legend()\n","\n","fig.add_subplot(122)\n","plt.plot(history['training_accuracy'], label='Train')\n","plt.plot(history['validation_accuracy'], label='Validation')\n","plt.xlabel('Epoch', size=12)\n","plt.ylabel('Categorical accuracy', size=12)\n","plt.title('Accuracy as a function of the epoch')\n","plt.legend()\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"vyax0iXJoiBo"},"source":["Both loss and accuracy improve over time, but the gap between training and validation suggests a slight overfitting."]},{"cell_type":"markdown","metadata":{"id":"Hry5ewydXxl7"},"source":["## 5. Evaluating the test performance of a CNN"]},{"cell_type":"markdown","metadata":{"id":"gFcmXjsFY0PS"},"source":["Now let's take a look at some model predictions!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nhv2uaVGY0PS"},"outputs":[],"source":["# Get the model probabilities\n","preds = ... ## <-- EDIT THIS LINE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZYnYyrN4Y0PS"},"outputs":[],"source":["# Plot some example predictions\n","\n","num_preds = preds.shape[0]\n","num_examples = 10\n","inx = np.random.choice(num_preds, num_examples, replace=False)\n","gs = {'width_ratios': [2, 1]}\n","\n","for i in inx:\n","    x_example = x_test[i]\n","    true_label = y_test[i][0]\n","    prediction = preds[i]\n","    pred_class = np.argmax(prediction)\n","    fig, ax = plt.subplots(figsize=(14, 1), ncols=2, gridspec_kw=gs)\n","    ax[0].imshow(x_example, cmap='Greys', vmin=-1, vmax=1)\n","    ax[0].set_ylabel('Reading')\n","    ax[0].set_xlabel('Timestep')\n","    ax[0].set_title('True label: {}\\nPredicted label: {}'.format(classes[true_label], classes[pred_class]))\n","    ax[1].bar(['Walk', 'Walk\\nUp', 'Walk\\nDown', 'Sit', 'Stand', 'Lay'], prediction)\n","    ax[1].set_title('Class predictions')\n","    ax[1].set_ylabel('Probability')"]},{"cell_type":"markdown","metadata":{"id":"66bHzMiGY0PS"},"source":["### Questions\n","1. Why have we not explicitly added a `Softmax` layer after the final linear layer of the CNN?\n","2. Are there particular classes where the model tends to be more uncertain in its predictions?\n","3. What effect does weight regularisation have on training and the final model? Apply regularisation to your model and experiment with different values of the regularisation coefficient to observe its impact.\n","4. How did the `max_patience` hyperparameter impact the training run above? What would have happened if we set `max_patience` to zero?\n","5. How does the training change when using the `SGD` optimiser instead of `Adam`?"]},{"cell_type":"markdown","metadata":{"id":"6WsFu7tooiBo"},"source":["### Answer to question 1"]},{"cell_type":"markdown","metadata":{"id":"DIInYFOzoiBo"},"source":["Let the model output be a vector of logits:\n","\n","$$\\mathbf{a} = [a_1, a_2, \\dots, a_Q],$$\n","\n","where $Q$ is the number of classes.\n","\n","`Softmax` converts logits into probabilities:\n","\n","$$P(y = c \\mid \\mathbf{a}) = \\frac{\\exp(a_c)}{\\sum_{q=1}^{Q}\\exp(a_q)}.$$\n","\n","However, we do not explicitly add a `Softmax` layer after the final linear layer in the CNN because PyTorch's `CrossEntropyLoss` already combines `Softmax` and Negative Log-Likelihood loss (`NLLLoss`). It directly applies `log_softmax`, which is mathematically equivalent but numerically more stable:\n","\n","$$\\log P(y = c \\mid \\mathbf{a}) = a_c - \\log \\sum_{q=1}^{Q} \\exp(a_q).$$\n","\n","The `CrossEntropyLoss` then computes the loss for a mini-batch of size $|S_m|$:\n","\n","$$L = -\\frac{1}{|S_m|} \\sum_{i\\in S_m}\\log P(y^{(i)} \\mid \\mathbf{a}^{(i)}) = - \\frac{1}{|S_m|} \\sum_{i\\in S_m}\\left(a^{(i)}_{y_i} - \\log \\sum_{q=1}^{Q} \\exp(a^{(i)}_{q}) \\right),$$\n","\n","where $a^{(i)}_{y_i}$ represents the logit (raw score) corresponding to the correct class $y^{(i)}$ for the $i$-th sample in the batch."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":0}