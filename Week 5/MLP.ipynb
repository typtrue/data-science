{"cells":[{"cell_type":"markdown","metadata":{"id":"NX_65NoOO6Qc"},"source":["# Multilayer Perceptron (MLP)\n","\n","The purpose of this notebook is to practice implementing the multilayer perceptron (MLP) model from scratch, focussing a regression task.\n","\n","The MLP is a type of neural network model that composes multiple affine transformations together, and applying pointwise nonlinearities in between. Equations for the hidden layers' activations $\\mathbf{h}^{(l)}\\in\\mathbb{R}^{n_l}$, $l=0,1,...,N_L$ ($N_L$ being the number of hidden layers in the MLP), and the output $\\hat{\\mathbf{y}}\\in\\mathbb{R}^{n_{N_{L}+1}}$ are given in the following:\n","\n","$$\n","\\begin{align}\n","\\mathbf{h}^{(0)} &:= \\mathbf{x},\\tag{1}\\\\\n","\\mathbf{a}^{(l)} &=  \\mathbf{W}^{(l-1)} \\mathbf{h}^{(l-1)} + \\mathbf{b}^{(l-1)},\\tag{2}\\\\\n","\\mathbf{h}^{(l)} &= \\sigma( \\mathbf{a}^{(l)}),\\qquad l=1,\\ldots, N_L,\\tag{3}\\\\\n","\\mathbf{\\hat{y}} &= \\sigma_{out}(\\mathbf{a}^{(N_{L}+1)}) = \\sigma_{out}\\left( \\mathbf{W}^{(N_L)} \\mathbf{h}^{(N_L)} + \\mathbf{b}^{(N_L)} \\right),\\tag{4}\n","\\end{align}\n","$$\n","\n","where $\\mathbf x\\in\\mathbb{R}^p$ is the input vector, $\\mathbf{W}^{(l)}\\in\\mathbb{R}^{n_{l+1}\\times n_{l}}$ are the weights, $\\mathbf{b}^{(l)}\\in\\mathbb{R}^{n_{l+1}}$ are the biases, $\\sigma, \\sigma_{out}: \\mathbb{R}\\mapsto\\mathbb{R}$ are activation functions that operate element-wise, $n_l$ is the number of units in the $l$-th hidden layer, and we have set $n_0 := p$. Moreover, $\\mathbf{a}^{(l)}$ is called the pre-activation and $\\mathbf{h}^{(l)}$ the post-activation (or activation) of layer $l$.\n","\n","#### Test Cases\n","\n","Some of the tasks in this notebook are followed by cells that you can use to verify your solution, to help you in the intermediate steps of this construction."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Raz3J-NF3OQd"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Imports used for testing\n","import numpy.testing as npt\n","\n","# Initialise random number generator\n","rng = np.random.default_rng(2)\n","\n","# We set a global configuration for matplotlib to obtain well-readable plots\n","SMALL_SIZE = 12\n","MEDIUM_SIZE = 16\n","BIGGER_SIZE = 20\n","\n","plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n","plt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the axes title\n","plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n","plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n","plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n","plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n","plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title"]},{"cell_type":"markdown","metadata":{"id":"gJVCL2IuQzY3"},"source":["## 1. Data preprocessing\n","\n","In this notebook we will use the California Housing data, which you have already seen in week 2 (kNN notebook)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8pOVoN4Y3OQ6"},"outputs":[],"source":["# load California Housing data\n","X = np.load('california_housing_X.npy')\n","y = np.load('california_housing_y.npy')"]},{"cell_type":"markdown","source":["**Data Set Characteristics:**\n","\n","Number of Instances: 20640\n","\n","Number of Attributes: 8 numeric, predictive attributes and the target\n","\n","Attribute Information:\n","- MedInc:        median income in block group\n","- HouseAge:      median house age in block group\n","- AveRooms:      average number of rooms per household\n","- AveBedrms:     average number of bedrooms per household\n","- Population:    block group population\n","- AveOccup:      average number of household members\n","- Latitude:      block group latitude\n","- Longitude:     block group longitude\n","\n","Missing Attribute Values: None\n","\n","This dataset was obtained from the StatLib repository.\n","https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n","\n","The target variable is the median house value for California districts,\n","expressed in hundreds of thousands of dollars ($100,000).\n","\n","This dataset was derived from the 1990 U.S. census, using one row per census\n","block group. A block group is the smallest geographical unit for which the U.S.\n","Census Bureau publishes sample data (a block group typically has a population\n","of 600 to 3,000 people).\n","\n","A household is a group of people residing within a home. Since the average\n","number of rooms and bedrooms in this dataset are provided per household, these\n","columns may take surprisingly large values for block groups with few households\n","and many empty houses, such as vacation resorts.\n","\n","References\n","\n","- Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n","  Statistics and Probability Letters, 33 (1997) 291-297"],"metadata":{"id":"BjGwYBGNovT5"}},{"cell_type":"markdown","metadata":{"id":"yBgz_tTOSOH1"},"source":["As usual, we create a train and test split of the data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hYKEPz04SVlf"},"outputs":[],"source":["# shuffling the rows in X and y\n","p = rng.permutation(len(y))\n","Xp = X[p]\n","yp = y[p]\n","\n","# we split train to test as 80:20\n","split_rate = 0.8\n","X_train, X_test = np.split(Xp, [int(split_rate*(Xp.shape[0]))])\n","y_train, y_test = np.split(yp, [int(split_rate*(yp.shape[0]))])"]},{"cell_type":"markdown","metadata":{"id":"bPPCblSQT_6f"},"source":["With neural network architectures, it is rather common to standardise the data, so we do it also here. It is important to standardise *after* applying the train-test split to prevent any information leakage from the test set into the train set, which can lead to over-optimistic results and unrealistic performance evaluations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gO_zwEHgYWDR"},"outputs":[],"source":["## EDIT THIS FUNCTION\n","def standardise(X, X_train_=None):\n","    \"\"\"Standardise features.\n","\n","    Parameters:\n","        X (np.array): Feature matrix.\n","        X_train_ (np.array): An optional feature matrix to compute the statistics\n","            from before applying it to X. If None, just use X to compute the statistics.\n","\n","    Returns:\n","        X_std (np.array): Standardised feature matrix\n","    \"\"\"\n","    if X_train_ is None:\n","        X_train_ = X\n","\n","    mu = np.mean(X_train_, axis=0, keepdims=True)\n","    sigma = np.std(X_train_, axis=0, keepdims=True)\n","    X_std = (X - mu) / sigma\n","    return X_std"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2CSVLJ-nT-Hx"},"outputs":[],"source":["# standardise train and test data\n","X_test = standardise(X_test, X_train_= X_train)\n","X_train = standardise(X_train)"]},{"cell_type":"markdown","metadata":{"id":"FAMrZQ9n3OQ5"},"source":["## 2. Implementation of MLP with 3 hidden layers\n","\n","Now you should implement an MLP regression model in numpy. The model will have three hidden layers with 64 neurons each, and using a ReLU activation function. The final output layer will be a single neuron with no activation function to predict the target variable $y$.\n","\n","The building block of the model is the dense (or fully-connected) layer. The following function should implement the affine transformation of this layer, given weights and bias parameters and the input to the layer. It should return the layer pre-activations (no activation function)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KEMpbrBS3OQ9"},"outputs":[],"source":["## EDIT THIS FUNCTION\n","def dense(X, W, b):\n","    \"\"\"Full-connected MLP layer.\n","\n","    Parameters:\n","        X (np.ndarray): K x h_in array of inputs, where K is the batch size and h_in is the input dimension.\n","        W (np.ndarray): h_out x h_in array for weights matrix parameters, where h_out is the output dimension.\n","        b (np.ndarray): Length h_out 1-D array for bias parameters\n","\n","    Returns:\n","        a (np.ndarray): K x h_out array of pre-activations\n","    \"\"\"\n","    a = np.vstack([... for x in X]) ## <-- EDIT THIS LINE\n","    return a"]},{"cell_type":"markdown","metadata":{"id":"7A29pjM23OQ-"},"source":["The hidden layers of our model will use a *ReLU* activation function, given by:\n","\n","$$\\sigma_{\\mathrm{ReLU}}(x)=\\max(0,x)\\tag{5}$$\n","\n","In the following cell, you should implement the *ReLU* activation and its gradient, which will be required in the backpropagation of the MLP."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y6YPF3_z3OQ-"},"outputs":[],"source":["## EDIT THIS FUNCTION\n","def relu_activation(a):\n","    \"\"\"ReLU activation function.\n","\n","    Parameters:\n","        a: K x h_out array of pre-activations\n","\n","    Returns:\n","        h: K x h_out array of post-activations\n","    \"\"\"\n","    # compute post-activations\n","    h = np.maximum(a, 0.)  ## <-- EDIT THIS LINE\n","    return h\n","\n","## EDIT THIS FUNCTION\n","def grad_relu_activation(a):\n","    \"\"\"Gradient of ReLU activation function.\n","\n","    Parameters:\n","        a: K x h_out array of pre-activations\n","\n","    Returns:\n","        grad: K x h_out gradient array of post-activations\n","    \"\"\"\n","    # compute gradient\n","    grad = ## <-- EDIT THIS LINE\n","    return grad"]},{"cell_type":"markdown","metadata":{"id":"LPV29hKc3OQ_"},"source":["Our MLP will need the following parameters:\n","\n","Input layer -> first hidden layer:\n","* Weights $\\mathbf{W}^{(0)} \\in\\mathbb{R}^{64 \\times 8}$\n","* Bias $\\mathbf{b}^{(0)} \\in\\mathbb{R}^{64}$\n","\n","Hidden layer -> hidden layer:\n","* Weights $\\mathbf{W}^{(l)} \\in\\mathbb{R}^{64\\times 64}$, $l=1, 2$\n","* Bias $\\mathbf{b}^{(l)} \\in\\mathbb{R}^{64}$, $l=1, 2$\n","\n","Hidden layer -> output layer:\n","* Weights $\\mathbf{W}^{(3)} \\in\\mathbb{R}^{1 \\times 64}$\n","* Bias $\\mathbf{b}^{(3)} \\in\\mathbb{R}^{1}$\n","\n","We will create these parameters as numpy arrays, and initialise the weights values with samples from a zero-mean Gaussian distribution with variance $2 / (n_{in} + n_{out})$, where $n_{in}$ and $n_{out}$ are the number of neurons going in and out of the dense layer respectively. This initialisation strategy is known as [Glorot initialisation](http://proceedings.mlr.press/v9/glorot10a.html). The bias parameters will be initialised to zeros."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DejTZWzh3ORA"},"outputs":[],"source":["# create the parameters using Glorot initialisation\n","var0 = 2. / (64 + 8)\n","W0 = rng.normal(size=(64, 8)) * np.sqrt(var0)\n","b0 = np.zeros(64)\n","\n","var1 = 2. / (64 + 64)\n","W1 = rng.normal(size=(64, 64)) * np.sqrt(var1)\n","b1 = np.zeros(64)\n","\n","var2 = 2. / (64 + 64)\n","W2 = rng.normal(size=(64, 64)) * np.sqrt(var2)\n","b2 = np.zeros(64)\n","\n","var3 = 2. / (1 + 64)\n","W3 = rng.normal(size=(1, 64)) * np.sqrt(var3)\n","b3 = np.zeros(1)"]},{"cell_type":"markdown","metadata":{"id":"RS03WI7J3ORB"},"source":["You should use these parameters and your `dense` function to create the MLP model. Remember that the hidden layers of the model should use a ReLU activation, and the output of the model should not use an activation function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hYINNJAb3ORC"},"outputs":[],"source":["## EDIT THIS FUNCTION\n","def mlp_3layers(X):\n","    \"\"\"MLP with 3 hidden layers and ReLU activation\n","\n","    Parameters:\n","        X: K x 8 array of inputs\n","\n","    Returns:\n","        y:  K x 1 output array\n","    \"\"\"\n","    if X.ndim == 1:\n","        # If one example passed, add a dummy dimension for the batch.\n","        X = X.reshape(1, -1)\n","\n","    # compose 3-layer MLP\n","    h = X\n","    a = dense(h, W0, b0)\n","    h = relu_activation(a)\n","\n","    a = dense(h, W1, b1)\n","    h = relu_activation(a)\n","\n","    a = dense(h, W2, b2)\n","    h = relu_activation(a)\n","    y = dense(h, W3, b3)\n","\n","    return y"]},{"cell_type":"markdown","metadata":{"id":"TgTZAog6PgGj"},"source":["We can now make a prediction with this simple MLP model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZJ7zSsmK3ORD"},"outputs":[],"source":["# Get the output of your initialised model\n","y_hat_train = mlp_3layers(X_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AqZZlCTOFu27"},"outputs":[],"source":["mlp_3layers(X_train[100])"]},{"cell_type":"markdown","metadata":{"id":"hZMEUxz1D5gZ"},"source":["**Verification:** To verify your implementation of `relu_activation`, `dense`, `standardise`, you should expect the following cells to execute without error messages."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3RtaRfCwd9tM"},"outputs":[],"source":["# The two lines shoud verify your implementation of\n","# relu_activation, dense, and standardise functions.\n","npt.assert_allclose(mlp_3layers(X_train[10]), 0.15248698)\n","npt.assert_allclose(mlp_3layers(X_train[100]), 0.12206486)"]},{"cell_type":"markdown","metadata":{"id":"Qpky3-o3muv4"},"source":["We also implement two alternative activation functions. The *sigmoid* activation function $\\sigma(x)$ is given by:\n","\n","$$\\sigma_{\\mathrm{sigmoid}}(x) = \\frac{1}{1+\\exp(-x)},\\tag{6}$$\n","\n","and the *tanh* activation is given by:\n","\n","$$\\sigma_{\\mathrm{tanh}}(x)=\\frac{\\exp(x)-\\exp(-x)}{\\exp(x)+\\exp(-x)}=\\frac{2}{1+\\exp(-2x)}-1\\tag{7}$$\n","\n","In the following two cells you need to implement both activation functions and their gradients."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TsK8QOqldyQw"},"outputs":[],"source":["## EDIT THIS FUNCTION\n","def tanh_activation(a):\n","    \"\"\"Tanh activation function.\n","\n","    Parameters:\n","        a: K x h_out array of pre-activations\n","\n","    Returns:\n","        h: K x h_out array of post-activations\n","    \"\"\"\n","    # compute post-activations\n","    h = ## <-- EDIT THIS LINE\n","    return h\n","\n","## EDIT THIS FUNCTION\n","def grad_tanh_activation(a):\n","    \"\"\"Gradient of Tanh activation function.\n","\n","    Parameters:\n","        a: K x h_out array of pre-activations\n","\n","    Returns:\n","        grad: K x h_out gradient array of post-activations\n","    \"\"\"\n","    # compute gradient\n","    grad = ## <-- EDIT THIS LINE\n","    return grad"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TWh0yKi9m2ti"},"outputs":[],"source":["## EDIT THIS FUNCTION\n","def sigmoid_activation(a):\n","    \"\"\"Sigmoid activation function. We implement here a numerically stable version\n","    where sigmoid(a) = 1/(1+exp(-a)) when a => 0 and sigmoid(a) = exp(a)/(1+exp(a))\n","    when a < 0.\n","\n","    Parameters:\n","        a: K x h_out array of pre-activations\n","\n","    Returns:\n","        h: K x h_out array of post-activations\n","    \"\"\"\n","\n","    # handle scalar inputs\n","    if np.isscalar(a):\n","        a = np.array([a])\n","\n","    # determine indices where a is positive or negative\n","    positive = a >= 0\n","    negative = ~positive\n","\n","    # compute sigmoid for positive a\n","    h = np.empty_like(a, dtype=float)\n","    h[positive] = ## <-- EDIT THIS LINE\n","\n","    # compute sigmoid for negative a\n","    exp = ## <-- EDIT THIS LINE\n","    h[negative] = ## <-- EDIT THIS LINE\n","\n","    return h\n","\n","## EDIT THIS FUNCTION\n","def grad_sigmoid_activation(a):\n","    \"\"\"Gradient of Sigmoid activation function.\n","\n","    Parameters:\n","        a: K x h_out array of pre-activations.\n","\n","    Returns:\n","        grad: K x h_out gradient array of post-activations.\n","    \"\"\"\n","    # compute gradient\n","    grad = ## <-- EDIT THIS LINE\n","    return grad"]},{"cell_type":"markdown","metadata":{"id":"DflYJX5wIb6W"},"source":["**Verification:** To verify your implementation of `tanh_activation`, `grad_tanh_activation`, `sigmoid_activation`, and `grad_sigmoid_activation` run the following cell:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qLyVJVnQIbN-"},"outputs":[],"source":["### Verification:\n","\n","# tanh (test against np.tanh)\n","npt.assert_allclose(tanh_activation(3), np.tanh(3))\n","npt.assert_allclose(tanh_activation(-3), np.tanh(-3))\n","\n","# sigmoid\n","npt.assert_allclose(sigmoid_activation(2.5),  0.9241418199787566)\n","npt.assert_allclose(sigmoid_activation(-3), 0.04742587317756678)\n","\n","# gradient of tanh\n","npt.assert_allclose(grad_tanh_activation(2), 0.07065082485316443)\n","npt.assert_allclose(grad_tanh_activation(-1), 0.41997434161402614)\n","\n","# gradient of sigmoid\n","npt.assert_allclose(grad_sigmoid_activation(2), 0.10499358540350662)\n","npt.assert_allclose(grad_sigmoid_activation(-1), 0.19661193324148185)"]},{"cell_type":"markdown","metadata":{"id":"lj8yKLKxXQt6"},"source":["## 3. Implementation of general MLP model\n","\n"]},{"cell_type":"markdown","metadata":{"id":"F1uF3al-KqHm"},"source":["To generalise our code, we write a MLP class where we can choose the number of layers and dimensions flexibly. (For background on object oriented programming in python see for example: https://www.geeksforgeeks.org/python-oops-concepts/.) Our strategy is to define a `layers` attribute that stores all the information on the layers as a list of dictionaries, with keys `\"W\"` corresponding to the weights $\\mathbf{W}^{(l)}\\in\\mathbb{R}^{n_{l+1}\\times n_{l}}$, `\"b\"` to the biases $\\mathbf{b}^{(l)}\\in\\mathbb{R}^{n_{l+1}}$ and `\"activation\"` for the activation functions $\\sigma^{(l)}: \\mathbb{R}\\mapsto\\mathbb{R}$. Below is an illustration for two layers. You will have to implement a method `add_layer` that adds a layer to to the MLP."]},{"cell_type":"markdown","metadata":{"id":"iG03l22q8RyV"},"source":["![layers](https://raw.githubusercontent.com/barahona-research-group/mfds-resources/main/images/mlp.drawio.svg)"]},{"cell_type":"markdown","metadata":{"id":"KwX55ENdCAgB"},"source":["Once the architecture of the MLP is defined one can make predictions by computing a forward pass. You will implement this in the `predict` method of the MLP class, which returns the predication and the all the pre-activations and post-activations in the forward pass (again as a list of dictionaries with keys `\"a\"` and `\"h\"` as visualised below). We need the information from the forward pass later for the backpropagation."]},{"cell_type":"markdown","metadata":{"id":"axJAXnMLCAjg"},"source":["![forward-pass](https://raw.githubusercontent.com/barahona-research-group/mfds-resources/main/images/mlp-forward.drawio.svg)"]},{"cell_type":"markdown","source":["We assume here that the output $\\mathbf{\\hat{y}}$ of the MLP is used for regression, i.e., we set the output activation $\\sigma_{out}$ in Eq. (4) to the identity function."],"metadata":{"id":"ElNWjOhssVBd"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"DMERegwRB_3A"},"outputs":[],"source":["# A lookup table for activation functions by their names.\n","activation_table = {\n","    \"relu\": relu_activation,\n","    \"sigmoid\": sigmoid_activation,\n","    \"tanh\": tanh_activation,\n","    # Identity function.\n","    \"identity\": lambda x: x\n","}\n","\n","# A lookup table for gradient of activation functions by their names.\n","grad_activation_table = {\n","    \"relu\": grad_relu_activation,\n","    \"sigmoid\": grad_sigmoid_activation,\n","    \"tanh\": grad_tanh_activation,\n","    # Identity function gradient.\n","    \"identity\": lambda x: np.ones_like(x)\n","}\n","\n","class MLP:\n","    \"\"\"\n","    This class represents a Multi-Layer Perceptron (MLP), that we are going\n","    to use to encapsulate two components:\n","        1. layers: the sequence of layers, where each layer is stored in\n","            a dictionary in the format {\"W\": np.ndarray, \"b\": np.ndarray},\n","            where \"W\" points to the weights array, and \"b\" points to\n","            the bias vector.\n","        2. rng: a pseudo random number generator (RNG) initialised to generate\n","            the random weights in a reproducible manner between different\n","            runtime sessions.\n","    This class is also shipped with methods that perform essential operations\n","    with a MLP, including:\n","        - add_layers: which creates a new layer with specified dimensions.\n","        - predict: applies the MLP forward pass to make predictions and produces\n","            a computational graph for the forward pass that can be used to\n","            compute gradients using backpropagation algorithm.\n","        in addition to other light functions that return simple statistics about\n","        the MLP.\n","    \"\"\"\n","    def __init__(self, seed=42):\n","        self.layers = []\n","        self.rng = np.random.default_rng(seed)\n","\n","    def n_parameters(self):\n","        \"\"\"Return the total number of parameters of weights and biases.\"\"\"\n","        return sum(l[\"b\"].size + l[\"W\"].size for l in self.layers)\n","\n","    def n_layers(self):\n","        \"\"\"Return current number of MLP layers.\"\"\"\n","        return len(self.layers) + 1 if len(self.layers) > 0 else 0\n","\n","    def layer_dim(self, index):\n","        \"\"\"Retrieve the dimensions of the MLP layer at `index`.\"\"\"\n","        return self.layers[index][\"W\"].shape\n","\n","    def add_layer(self, in_dim, out_dim, activation=\"identity\"):\n","        \"\"\"Add fully connected layer to MLP.\n","\n","        Parameters:\n","            in_dim (int): The input dimension of the layer.\n","            out_dim (int): The output dimension of the layer.\n","            activation (str): The activation function name.\n","        \"\"\"\n","        # check if input-dimension matches output-dimension of previous layer\n","        if self.n_layers() > 0:\n","            last_out_dim, _ = self.layer_dim(-1)\n","            assert in_dim == last_out_dim, f\"Input-dimension {in_dim} does not match output-dimension {last_out_dim} of previous layer.\"\n","\n","        # the first layer, in our convention illustrated, does not apply activation on the input features X.\n","        if self.n_layers() == 0:\n","            assert activation == \"identity\", \"Should not apply activations on the input features X, use Identity function for the first layer.\"\n","\n","\n","        # store each layer as a dictionary in the list, as shown in the\n","        # attached diagram.\n","        self.layers.append({\n","            # only for debugging.\n","            \"index\": len(self.layers),\n","            # apply Glorot initialisation for weights.\n","            # hint: use self.rng.normal()\n","            \"W\": ## <-- EDIT THIS LINE\n","            # initialise bias vector with zeros.\n","            \"b\": ## <-- EDIT THIS LINE\n","            # store the activation function (as string)\n","            \"activation\": activation\n","        })\n","\n","    def predict(self, X):\n","        \"\"\"Apply the forward pass on the input X and produce prediction and the\n","        forward computation graph.\n","\n","        Parameters:\n","            X (np.ndarray): Feature matrix.\n","\n","        Returns:\n","            (np.ndarray, List[Dict[str, np.ndarray]]): A tuple of the\n","            predictions and the computation graph as a sequence of intermediate\n","            values through the MLP, specifically each layer will have a corresponding\n","            intermediate values {\"a\": np.ndarray, \"h\": np.ndarray}, as shown in the\n","            attached diagram above.\n","        \"\"\"\n","        # We assume that we work with a batch of examples (ndim==2).\n","        if X.ndim == 1:\n","            # If one example passed, add a dummy dimension for the batch.\n","            X = X.reshape(1, -1)\n","\n","        # store pre- and post-activations in list\n","        forward_pass = [{\"index\": 0, \"a\": X, \"h\": X}]\n","\n","        # iterate through hidden layers\n","        for k in range(1, len(self.layers)):\n","            # compute pre-activations\n","            a = ## <-- EDIT THIS LINE\n","            activation = ## <-- EDIT THIS LINE\n","            forward_pass.append({\"index\": k, \"a\" : a, \"h\" : activation(a)})\n","\n","        y_hat = ## <-- EDIT THIS LINE\n","        # predicted target is output of last layer\n","        return y_hat, forward_pass"]},{"cell_type":"markdown","metadata":{"id":"e9_2H_90L2MM"},"source":["We can now implement the same MLP architecture as before by creating an object from the `MLP` class, initialised with `seed=2`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VrjkCH9PdnIP"},"outputs":[],"source":["mlp = MLP(seed=2)\n","mlp.add_layer(8, 64)\n","mlp.add_layer(64, 64, \"relu\")\n","mlp.add_layer(64, 64, \"relu\")\n","mlp.add_layer(64, 1, \"relu\")\n","print(\"Number of layers (including input and output layer):\", mlp.n_layers())\n","print(\"Number of trainable parameters:\", mlp.n_parameters())"]},{"cell_type":"markdown","metadata":{"id":"tQBfNuJEG3bZ"},"source":["**Test Cases** Now we may verify the `MLP` implementation with input examples with expected outputs. If you struggle with getting it through, you can start with simpler test cases. For example, you can start from 1-layer and a toy two dimensional input and test against what you expect by manual derivation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FN2FucSY1lq1"},"outputs":[],"source":["mlp = MLP(seed=2)\n","mlp.add_layer(8, 64)\n","mlp.add_layer(64, 64, \"relu\")\n","mlp.add_layer(64, 64, \"relu\")\n","mlp.add_layer(64, 1, \"relu\")\n","\n","\n","case_input = X_train[:5]\n","case_expect = np.array([-0.03983787, -0.04034381,  0.14796773, -0.02884643,  0.03103732])\n","npt.assert_allclose(mlp.predict(case_input)[0].squeeze(), case_expect)"]},{"cell_type":"markdown","metadata":{"id":"NMNLSVfXIQpq"},"source":["## 4. Backpropagation in MLP"]},{"cell_type":"markdown","metadata":{"id":"sf1nEVpYQHyU"},"source":["We use the MSE loss function to evaluate the performance of our model. You will implement it in the cell below, together with its gradient."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gaIxPxn5M-Oc"},"outputs":[],"source":["def mse_loss(y_true, y_pred):\n","    \"\"\"Compute MSE-loss\n","\n","    Parameters:\n","        y_true: ground-truth array, with shape (K, )\n","        y_pred: predictions array, with shape (K, )\n","\n","    Returns:\n","        loss (float): MSE-loss\n","    \"\"\"\n","    assert y_true.size == y_pred.size, \"Ground-truth and predictions have different dimensions.\"\n","\n","    # Adjustment to avoid subtraction between (K,) and (1, K) arrays.\n","    y_true = y_true.reshape(y_pred.shape)\n","\n","    # Compute MSE loss\n","    loss = ## <-- EDIT THIS LINE\n","    return loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jyqtb5P1j304"},"outputs":[],"source":["def grad_mse_loss(y_true, y_pred):\n","    \"\"\"Compute gradient of MSE-loss\n","\n","    Parameters:\n","        y_true: ground-truth values, shape: (K, ).\n","        y_pred: prediction values, shape: (K, ).\n","\n","    Returns:\n","        grad (np.ndarray): Gradient of MSE-loss, shape: (K, ).\n","    \"\"\"\n","    # Adjustment to avoid subtraction between (K,) and (1, K) arrays.\n","    y_true = y_true.reshape(y_pred.shape)\n","\n","    # Compute gradient of MSE loss\n","    grad = ## <-- EDIT THIS LINE\n","    return grad"]},{"cell_type":"markdown","metadata":{"id":"UoMu1tckK6_V"},"source":["**Verification:** Let's test the implemented `mse_loss` and `grad_mse_loss`.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"njtxQ06UK20e"},"outputs":[],"source":["# We will use the same test inputs for mse_loss and grad_mse_loss.\n","case_input_arg1 = y_train[:5]\n","case_input_arg2 = np.array([-0.03983787, -0.04034381,  0.14796773, -0.02884643,  0.03103732])\n","\n","case_expects = 10.791119\n","npt.assert_allclose(mse_loss(case_input_arg1, case_input_arg2),\n","                    case_expects, atol=1e-4)\n","\n","case_expects = np.array([-2.01593915, -1.72533752, -0.34321291, -0.73353857, -0.96758507])\n","npt.assert_allclose(grad_mse_loss(case_input_arg1, case_input_arg2),\n","                    case_expects, atol=1e-4)"]},{"cell_type":"markdown","metadata":{"id":"4lSpUAHhQMxu"},"source":["In order to train our MLP model with stochastic gradient descent (SGD) we need to compute the gradients of all model parameters for a given minibatch. To do so we use the backpropagation algorithm explained in the lecture notes. Below you will implement a `backpropagate` function that returns for each layer the gradients of its weights and biases stored in a list of dictionaries with keys `\"W\"` and `\"b\"`. This data structure is visualised in the following:"]},{"cell_type":"markdown","metadata":{"id":"6tyrbfnPCtR7"},"source":["![backpropagation](https://raw.githubusercontent.com/barahona-research-group/mfds-resources/refs/heads/main/images/mlp-back_7FEB25.drawio.svg)"]},{"cell_type":"markdown","source":["Note that $\\delta^{(3)}_k$ for the output neurons in the illustration above corresponds to the gradient of the MSE loss. In general, you compute: $$\\delta^{(N_L)}=\\frac{\\partial L}{\\partial \\mathbf{a}^{(N_{L}+1)}},\\tag{8}$$\n","which in this case is equivalent to $\\frac{\\partial L}{\\partial \\mathbf{\\hat{y}}}$, since we have taken $\\sigma_{out}$ in Eq. (4) equal to the identity function."],"metadata":{"id":"sVppwzgqxx3L"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ad5YJ0AabpaJ"},"outputs":[],"source":["def backpropagate(layers, forward_pass, delta_output):\n","    \"\"\"\n","    Apply the backpropagation algorithm to the MLP layers to compute the gradients starting from\n","    the output layer to the input layer, and starting the chain rule from the\n","    partial derivative of the loss function w.r.t the predictions $\\hat{y}$\n","\n","    Parameters:\n","        layers (List[Dict[str, np.ndarray]]): The MLP sequence of layers, as shown in the diagrams.\n","        forward_pass (List[Dict[str, np.ndarray]]): The forward pass intermediate values for\n","            each layer, representing a computation graph.\n","        delta_output (np.ndarray): the partial derivative of the loss function w.r.t the\n","            predictions $\\hat{y}$, has the shape (K, 1), where K is the batch size.\n","    Returns:\n","        gradients (List[Dict[str, np.ndarray]]): The computed gradient using a structure symmetric in the layers, as shown\n","            in the diagrams.\n","\n","    \"\"\"\n","    #Â Create a list that will contain the gradients of all the layers.\n","    gradients = []\n","\n","    # Initialise delta.\n","    delta = delta_output\n","\n","    assert len(layers) == len(forward_pass), \"Number of layers is expected to match the number of forward pass layers\"\n","\n","    # Iterate on layers backwardly, from output to input.\n","    # Calculate gradients w.r.t. weights and biases of each level and store in list of dictionaries.\n","    for layer, forward_computes in reversed(list(zip(layers, forward_pass))):   # zip iterates through pairs of layers and forward_pass\n","        assert forward_computes[\"index\"] == layer[\"index\"], \"Mismatch in the index.\"\n","\n","        h = forward_computes[\"h\"]\n","        assert delta.shape[0] == h.shape[0], \"Mismatch in the batch dimension.\"\n","\n","        # Gradients are average gradients over batch\n","        gradients.append({\"W\" : ## <-- EDIT THIS LINE\n","                          \"b\" : ## <-- EDIT THIS LINE\n","\n","        # Update the delta for the next iteration\n","        grad_activation_f = grad_activation_table[layer[\"activation\"]]\n","        grad_activation = grad_activation_f(forward_computes[\"a\"])\n","\n","        # Calculate the delta for the backward layer.\n","        delta = np.stack([... for (gi, di) in zip(grad_activation, delta)]) ## <-- EDIT THIS LINE\n","\n","\n","    # Return now ordered list matching the layers.\n","    gradients = list(reversed(gradients))\n","    return gradients"]},{"cell_type":"markdown","metadata":{"id":"rvhflMcY1lq7"},"source":["**Verification:** Here we conclude our tests for the implemented `MLP` by testing the gradients computed with backpropagation algorithm, which is considered the most delicate part to implement properly."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lZl3Np991lq8"},"outputs":[],"source":["my_mlp_test = MLP(seed=42)\n","my_mlp_test.add_layer(8, 3)\n","my_mlp_test.add_layer(3, 1, \"relu\")\n","\n","def test_grads(mlp, X, y):\n","    y_hat, forward_pass = mlp.predict(X)\n","    delta_output_test = grad_mse_loss(y, y_hat)\n","    return backpropagate(my_mlp_test.layers, forward_pass,\n","                                  delta_output_test)\n","\n","## Test the last layer gradients with a batch of size one.\n","grad = test_grads(my_mlp_test, X_train[0], y_train[0])\n","grad_W1 = grad[1][\"W\"]\n","npt.assert_allclose(grad_W1, np.array([[-0.05504819, -3.14611296, -0.22906985]]), atol=1e-4)\n","\n","# # Test the last layer gradients with a batch of size two\n","grad = test_grads(my_mlp_test, X_train[0:2], y_train[0:2])\n","grad_W1 = grad[1][\"W\"]\n","npt.assert_allclose(grad_W1, np.array([[-0.01376205, -0.78652824, -0.05726746]]), atol=1e-4)\n","\n","# Test the first layer bias with the same batch\n","grad = test_grads(my_mlp_test, X_train[0:2], y_train[0:2])\n","grad_b0 = grad[0][\"b\"]\n","npt.assert_allclose(grad_b0, np.array([ 0.76784506,  0.63125483, -0.95424802]), atol=1e-4)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"cNTAX1fvQWHc"},"source":["The implementation of backpropagation can be challenging and we will go through all the steps in the tutorials."]},{"cell_type":"markdown","metadata":{"id":"lAUzjw9nTZyx"},"source":["## 5. Training the MLP with mini-batch Stochastic Gradient Descent"]},{"cell_type":"markdown","metadata":{"id":"EGZf-4vcQY_B"},"source":["After computing the gradients of weights for a minibatch we can update the weights according to the mini-batch SGD algorithm explained in the lecture notes. The `sgd_step` function below implements this for a single minibatch."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OiFJlYhxTdCZ"},"outputs":[],"source":["def sgd_step(X, y, mlp, learning_rate = 1e-3):\n","    \"\"\"\n","    Apply a stochastic gradient descent step using the sampled batch.\n","    Parameters:\n","        X (np.ndarray): The input features array batch, with dimension (K, p).\n","        y (np.ndarray): The ground-truth of the batch, with dimension (K, 1).\n","        learning_rate (float): The learning rate multiplier for the update steps in SGD.\n","    Returns:\n","        updated_layers (List[Dict[str, np.ndarray]]): The updated layers after applying SGD.\n","    \"\"\"\n","    # Compute the forward pass.\n","    y_hat, forward_pass = ## <-- EDIT THIS LINE\n","\n","    # Compute the partial derivative of the loss w.r.t. to predictions `y_hat`.\n","    delta_output = ## <-- EDIT THIS LINE\n","\n","    # Apply backpropagation algorithm to compute the gradients of the MLP parameters.\n","    gradients = backpropagate(mlp.layers, forward_pass, delta_output)  ## <-- SOLUTION.\n","\n","    # mlp.layers and gradients are symmetric, as shown in the figure.\n","    updated_layers = []\n","    for layer, grad in zip(mlp.layers, gradients):\n","        W = ## <-- EDIT THIS LINE\n","        b = ## <-- EDIT THIS LINE\n","        updated_layers.append({\"W\": W, \"b\": b,\n","                               # keep the activation function.\n","                               \"activation\": layer[\"activation\"],\n","                               # We use the index for asserts and debugging purposes only.\n","                               \"index\": layer[\"index\"]})\n","    return updated_layers"]},{"cell_type":"markdown","source":["To assess the regression performance we also implement the $R^2$ score."],"metadata":{"id":"tzuD_5o5pLXr"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ApSQylNg2h_d"},"outputs":[],"source":["def r2_score(y, y_hat):\n","    \"\"\"R^2 score to assess regression performance.\"\"\"\n","\n","    # Adjustment to avoid subtraction between (K,) and (1, K) arrays.\n","    y = y.reshape(y_hat.shape)\n","    y_bar = y.mean()\n","\n","    ss_tot = ((y - y_bar)**2).sum()\n","    ss_res = ((y - y_hat)**2).sum()\n","    return 1 - (ss_res/ss_tot)"]},{"cell_type":"markdown","metadata":{"id":"SXchjvr-QiI7"},"source":["A full training of the parameters over several epochs with SGD is then implemented in the next cell."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"68H26z2e9ujN"},"outputs":[],"source":["def sgd(X_train, y_train, X_test, y_test, mlp, learning_rate = 1e-3,\n","        n_epochs=10, minibatchsize=1, seed=42):\n","    \"\"\"\n","    Run the Stochastic Gradient Descent (SGD) algorithm to optimise the parameters of MLP model to fit it on\n","    the training data using MSE loss.\n","\n","    Parameters:\n","        X_train (np.ndarray): The training data features, with shape (N^{training}, p).\n","        y_train (np.ndarray): The training data ground-truth, with shape (N^{training}, 1).\n","        X_test (np.ndarray): The testing data features, with shape (N^{test}, p).\n","        y_test (np.ndarray): The testing data ground-truth, with shape (N^{test}, 1).\n","        mlp (MLP): The MLP object enacpsulating the MLP model.\n","        learning_rate (float): The learning_rate multiplier used in updating the parameters at each iteration.\n","        n_epochs (int): The number of training cycles that each covers the entire training examples.\n","        minibatchsize (int): The batch size used in each SGD step.\n","        seed (int): A seed for the RNG to ensure reproducibility across runtime sessions.\n","\n","    Returns:\n","        mlp (MLP): MLP object encapuslating the trained MLP model.\n","        losses_train (np.ndarray): Train losses over epochs.\n","        losses_tset (np.ndarray): Test losses over epochs.\n","    \"\"\"\n","\n","    # get random number generator\n","    rng = np.random.default_rng(seed)\n","\n","    # compute number of iterations per epoch\n","    n_iterations = int(len(y_train) / minibatchsize)\n","\n","    # store losses\n","    losses_train = []\n","    losses_test = []\n","\n","    for i in range(n_epochs):\n","\n","        # shuffle data\n","        p = rng.permutation(len(y_train))\n","        X_train_shuffled = X_train[p]\n","        y_train_shuffled = y_train[p]\n","\n","        for j in range(n_iterations):\n","            # get batch\n","            X_batch = X_train_shuffled[j*minibatchsize : (j+1)*minibatchsize]\n","            y_batch = y_train_shuffled[j*minibatchsize : (j+1)*minibatchsize]\n","\n","            # apply sgd step\n","            updated_layers = ## <-- EDIT THIS LINE\n","\n","            # update weights and biases of MLP\n","            mlp.layers = ## <-- EDIT THIS LINE\n","\n","        # compute loss at the end of each epoch\n","        y_hat_train, _ = mlp.predict(X_train)\n","        loss_train = mse_loss(y_train, y_hat_train).squeeze()\n","        losses_train.append(loss_train)\n","        y_hat_test, _ = mlp.predict(X_test)\n","        loss_test = mse_loss(y_test, y_hat_test).squeeze()\n","        losses_test.append(loss_test)\n","\n","        if (i==0) or ((i+1)%50==0):\n","          print(\n","              f'Epoch {i+1}/{n_epochs}: In-sample error: {loss_train}, Out-of-sample error: {loss_test},'\n","              f'train R^2: {r2_score(y_train, y_hat_train):.2f}, test R^2: {r2_score(y_test, y_hat_test):.2f}.'\n","               )\n","\n","    return mlp, losses_train, losses_test\n"]},{"cell_type":"markdown","metadata":{"id":"PldRZ0AnQwCw"},"source":["In the following, train a small MLP model with the following architecture:\n","- Input: 8 neurons corresponding to features.\n","- First hidden layer: 16 neurons and ReLu activation.\n","- Second hidden layer: 8 neurons and Sigmoid activation.\n","- Third hidden layer: 4 neurons and ReLu activation.\n","- Ouput: 1 neuron corresponding to target.\n","\n","Use mini-batch SGD with learning rate 0.05, minibatch size 64 and train the MLP for 50 epochs.\n","\n","As before we start by creating a new object from the `MLP` class initialised with `seed=2`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"scVxSQuJ28Xl"},"outputs":[],"source":["# compile architecture of MLP\n","mlp = MLP(seed=2)\n","... ## <-- EDIT THIS LINE\n","... ## <-- EDIT THIS LINE\n","... ## <-- EDIT THIS LINE\n","... ## <-- EDIT THIS LINE\n","print(\"Number of layers (including input and output layer):\",mlp.n_layers())\n","print(\"Number of trainable parameters:\",mlp.n_parameters())\n","\n","# train MLP using SGD\n","mlp, losses_train, losses_test = ## <-- EDIT THIS LINE"]},{"cell_type":"markdown","metadata":{"id":"KSX_KxF2Qx-e"},"source":["We can plot the progress of the training below."]},{"cell_type":"code","source":["# plot training progress\n","fig, ax = plt.subplots(figsize=(12, 8))\n","ax.plot(np.arange(1,51),losses_train, label=\"Train\")\n","ax.plot(np.arange(1,51),losses_test, label=\"Test\")\n","ax.set(title=\"Losses vs Epochs\", xlabel = \"Epoch\", ylabel = \"MSE\")\n","ax.legend()\n","plt.show()"],"metadata":{"id":"9Z0z2ny9buvs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E3zduFv_3ORM"},"source":["#### Questions\n","1. How do the training and test losses above compare?\n","2. Play around with different numbers of hidden layers and number of neurons in the MLP. How is the performance of the model affected?\n","3. Play around with different activation functions. How is the performance of the model affected?"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.17"}},"nbformat":4,"nbformat_minor":0}