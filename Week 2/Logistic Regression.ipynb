{"cells":[{"cell_type":"markdown","metadata":{"id":"jvXEzGMxJY_0"},"source":["#### Prerequisites\n","\n","- Basic familiarity with [Numpy](https://numpy.org/doc/stable/user/quickstart.html)\n","- Basic familiarity with [Pyplot](https://matplotlib.org/stable/tutorials/introductory/pyplot.html)"]},{"cell_type":"markdown","metadata":{"id":"Bg_rI-LdJYyg"},"source":["<a name=\"outline\"></a>\n","\n","## Outline\n","\n","- [Section 1](#section-1): Intro to Logistic Regression\n","- [Section 2](#section-2): Dataset Generation\n","- [Section 3](#section-3): Logistic Parametric Function\n","- [Section 4](#section-4): Optimising Logistic Regression Parameters\n","- [Section 5](#section-5): Model evaluation\n","- [Extra 1](#extra-1): Receiver Operating Characteristic (ROC)"]},{"cell_type":"markdown","source":["# Logistic regression\n","The purpose of this notebook is to understand and implement logistic regression, again without using any package that has a complete logistic regression framework implemented (*e.g.*, scikit-learn).\n"],"metadata":{"id":"vqZ-2T7-huA0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"pk6OgGZ391A_"},"outputs":[],"source":["# Importing standard packages\n","import numpy as np\n","import matplotlib.pyplot as plt\n","#from sklearn.datasets import make_classification\n","\n","SMALL_SIZE = 12\n","MEDIUM_SIZE = 16\n","BIGGER_SIZE = 20\n","plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n","plt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the axes title\n","plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n","plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n","plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n","plt.rc('legend', fontsize=MEDIUM_SIZE)    # legend fontsize\n","plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n","\n","# fixing random generator for reproducibility\n","rng = np.random.default_rng(0)"]},{"cell_type":"markdown","source":["If you run this notebook locally on your machine, you will simply need to place the `csv` file in the same directory as this notebook.\n","If you run this notebook on Google Colab, you will need to\n","\n","  `from google.colab import files`\n","\n","  `upload = files.upload()`\n","\n","and then upload it from your local downloads directory."],"metadata":{"id":"lQuRt1Zzq48J"}},{"cell_type":"code","source":["from google.colab import files\n","\n","upload = files.upload()"],"metadata":{"id":"2QKibCL1q9HM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"girxRWz391Xo"},"source":["\n","\n","\n","<a name=\"section-1\"></a>\n","\n","## Section 1: Intro to Logistic Regression [^](#outline)\n","\n","\n","Logistic regression, despite its name, is a linear model for classification rather than regression. In its original form, it is used for binary classifications, i.e., assigning a data point in our test set a binary label (*e.g.*, yes or no, 0 or 1, red or blue). The reason why the term logistic *regression* is used becomes obvious once we examine the logistic function (often also called sigmoid function):\n","$$\n","h(z) = \\frac{1}{1+e^{-z}}\n","$$\n","Next, you will implement the logistic function using numpy."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zf6-RkVcRGeW"},"outputs":[],"source":["## EDIT THIS FUNCTION\n","def logistic(z):\n","    return ...  ## <-- EDIT THIS LINE"]},{"cell_type":"markdown","metadata":{"id":"eF8XcJUjBovk"},"source":["Let's plot the function to see how it behaves."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h2qqiNot906N"},"outputs":[],"source":["z = np.linspace(-6, 6, 1000)\n","y = logistic(z)\n","plt.xlabel(r'$z$')\n","plt.ylabel(r'$h(z)$')\n","plt.title('Sigmoid function')\n","plt.grid(alpha=0.5)\n","plt.plot(z, y);"]},{"cell_type":"markdown","metadata":{"id":"ioR-FjI0cF-B"},"source":["#### Questions:\n","1. Can you already explain why this _regression_ model is used in _binary classification_ tasks?\n","2. What do the bounds of the logistic function tell you?\n","\n","\n","<a name=\"section-2\"></a>\n","\n","## Section 2: Dataset Generation [^](#outline)\n","\n","We use a dataset that generated with $p=2$ features using sklearn's make_classification function. The dataset we use is accessible on Blackboard."]},{"cell_type":"code","source":["# The code below is how the data was generated, but we use the data given on Blackboard.\n","# make a dataset with 3 classes, 2 features and 100 samples\n","# X_f, y = make_classification(n_samples=500, n_features=2, n_redundant=0, n_informative=2, n_clusters_per_class=1, random_state=14)"],"metadata":{"id":"eEWVAriVrNWv"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oOtP1llv90m7"},"outputs":[],"source":["X_f = np.load('X_classification_LR.npy')\n","y = np.load('y_classification_LR.npy')\n","\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.title('2-class dataset')\n","plt.scatter(X_f[:,0], X_f[:,1], c=y.reshape(-1), cmap='bwr');"]},{"cell_type":"markdown","metadata":{"id":"3AGfIQXMc38A"},"source":["We divide the dataset into training and test set and run our model with our own choice of hyperparameters:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yxvfN7yI90kG"},"outputs":[],"source":["# Adding a first column of ones allowing for a bias/intercept term\n","X = np.hstack((np.ones((X_f.shape[0], 1)), X_f))\n","\n","# Stacking data 'X' and labels 'y' into one matrix\n","data = np.hstack((X, y[:, np.newaxis]))\n","\n","# Shuffling the rows\n","rng.shuffle(data)\n","\n","# Splitting into training and test in a 70-30 ratio\n","split_rate = 0.7\n","train, test = np.split(data, [int(split_rate*(data.shape[0]))])\n","\n","X_train = train[:,:-1]\n","y_train = train[:, -1]\n","\n","X_test = test[:,:-1]\n","y_test = test[:, -1]\n","\n","y_train = y_train.astype(int)\n","y_test = y_test.astype(int)"]},{"cell_type":"markdown","metadata":{"id":"62PHeY3vOLkw"},"source":["<a name=\"section-3\"></a>\n","\n","## Section 3: Logistic Parametric Function [^](#outline)"]},{"cell_type":"markdown","metadata":{"id":"oY1-6VPECXbt"},"source":["In logistic regression, we estimate the parameter vector $\\boldsymbol \\beta=(\\beta_0, \\beta_1, \\dots, \\beta_p)^T \\in \\mathbb R^{p+1}$ as in linear regression. However, the final step involves passing the output through a logistic function. We call the output of this operation $h_{\\boldsymbol \\beta}(\\boldsymbol x)$:\n","\n","$$\n","h_{\\boldsymbol \\beta}(\\boldsymbol x) := h(\\boldsymbol x^T \\boldsymbol \\beta)\n","$$\n","\n","where $\\boldsymbol x^T$ is an input datapoint, represented by a $(p+1)$ length vector $(1,x_1,\\ldots, x_p)$.\n","\n","Note that $h$ is again the logistic function and, consequently, we have a _probability_ of the given data point belonging to one of the two classes, such as red or blue. To label the data points, we can designate those with a probability above $0.5$ as red and those with a probability of $0.5$ or below as blue.\n","\n","In the following cell, write an implementation of $h_{\\boldsymbol \\beta}(\\boldsymbol x^T)$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XWv2CMYWRWVH"},"outputs":[],"source":["## EDIT THIS FUNCTION\n","def predict_log(X, beta):\n","    assert X.shape[1] == beta.shape[0] # expect X to have same number of columns as beta\n","    y_log = ...  ## <-- EDIT THIS LINE\n","    return y_log.squeeze()"]},{"cell_type":"markdown","metadata":{"id":"PrSoE2UsPeO0"},"source":["<a name=\"section-4\"></a>\n","\n","## Section 4: Optimising Logistic Regression Parameters [^](#outline)"]},{"cell_type":"markdown","metadata":{"id":"lFfYovQcPnh7"},"source":["### Parameter Initialisation"]},{"cell_type":"markdown","metadata":{"id":"I2_zR28_Ng1T"},"source":["A common technique in Machine Learning is to initialise the parameter vector $\\boldsymbol \\beta$ randomly or with zeros; we do the latter here."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cb9smGSNNuiT"},"outputs":[],"source":["def initialise(size):\n","    \"\"\"\n","    Argument:\n","    size: Size of the parameter vector beta\n","\n","    Returns:\n","    beta: Initialised vector of shape (size, 1)\n","\n","    \"\"\"\n","\n","    beta = np.zeros((size, 1))\n","\n","    return beta"]},{"cell_type":"markdown","metadata":{"id":"0OUdh27IPsvS"},"source":["### Parameter Optimisation"]},{"cell_type":"markdown","metadata":{"id":"tLEUlU6mLQ5G"},"source":["From the lecture notes, we know that the logistic regression parameters can be estimated by maximising the log-likelihood function. We can then consider the **mean negative log-likelihood** loss function and **minimise** the associated **mean sample loss**:\n","\n","$$\n","E(L) = - \\frac{1}{N}\\sum_{i=1}^N y^{(i)} \\log h_{\\boldsymbol \\beta}(\\boldsymbol x^{(i)}) + (1-y^{(i)}) \\log (1-h_{\\boldsymbol \\beta}(\\boldsymbol x^{(i)}))\n","$$\n","Which has as gradient\n","$$\n","\\nabla_{\\boldsymbol \\beta} E(L) = \\frac{1}{N}\\sum_{i=1}^N (h_{\\boldsymbol \\beta}(\\boldsymbol x^{(i)}) - y^{(i)})\\boldsymbol x^{(i)}\n","$$\n","\n","Implement the mean sample loss function and its gradient in the next cell as part of a larger operation which we shall call `propagate`, often also called a _forward pass_."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jr3G6ms_Rhhw"},"outputs":[],"source":["## EDIT THIS FUNCTION\n","def propagate(X, y, beta):\n","    \"\"\"\n","    This function computes the mean sample loss and its gradient.\n","\n","    Arguments:\n","    X: Data of shape (N, p+1)\n","    y: True label vector of size N\n","    beta: Parameter vector, a numpy array of size p+1\n","\n","    Returns:\n","    mean_loss: Mean sample loss for the negative log-likelihood\n","    dbeta: Gradient of the mean sample loss with respect to beta\n","\n","    \"\"\"\n","\n","    y_log = predict_log(X, beta)\n","\n","    assert y_log.shape == y.shape\n","\n","    # Mean sample loss function\n","    mean_loss = ...  ## <-- EDIT THIS LINE\n","\n","    # Derivatives\n","    dbeta = ...  ## <-- EDIT THIS LINE\n","\n","    mean_loss = np.squeeze(mean_loss)\n","\n","    # Store gradients in a dictionary\n","    grads = {'dbeta': dbeta}\n","\n","    return grads, mean_loss"]},{"cell_type":"markdown","metadata":{"id":"l5PbMBTDVkRi"},"source":["We can now conduct the actual optimisation and update the $\\boldsymbol \\beta$ with a learning rate $\\alpha$, which we shall set to $0.1$. You are required to implement the updating procedure for $\\boldsymbol \\beta$:\n","\n","$$\n","\\boldsymbol \\beta := \\boldsymbol \\beta - \\alpha \\nabla_{\\boldsymbol \\beta}  E(L)\n","$$\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wzX1fIIcBySX"},"outputs":[],"source":["## EDIT THIS FUNCTION\n","def optimise(X, y, beta, num_iterations=1000, learning_rate=0.1, print_loss=False):\n","    \"\"\"\n","    This function implements gradient descent to learn the parameters of the logistic regression model.\n","\n","    Arguments:\n","    X: Data of shape (N, p+1)\n","    y: True label vector of size N\n","    beta: Parameter vector, a numpy array of size p+1\n","    num_iterations: Number of iterations\n","    learning_rate: Step size in updating procedure\n","    print_loss: 'True' to print the mean loss every 100 iterations\n","\n","    Returns:\n","    params: Dictionary containing the parameter vector beta\n","    grads: Dictionary containing the gradient\n","    mean_loss_history: List of all the mean loss values computed during the optimisation (can be used to plot the learning curve)\n","\n","    \"\"\"\n","    mean_loss_history = []\n","\n","    for i in range(num_iterations):\n","\n","        # Calculating the loss and gradients (hint: use your existing functions)\n","        grads, mean_loss = ...  ## <-- EDIT THIS LINE\n","\n","        # Retrieving derivatives from grads\n","        dbeta = grads['dbeta']\n","\n","        # Updating procedure\n","        beta = ...  ## <-- EDIT THIS LINE\n","\n","        # Record the loss values\n","        if i % 100 == 0:\n","            mean_loss_history.append(mean_loss)\n","\n","        # Printing the loss every 100 iterations\n","        if print_loss and i % 100 == 0:\n","            print ('Mean loss after iteration %i: %f' %(i, mean_loss))\n","\n","    # Saving parameters and gradients in dictionary\n","    params = {'beta': beta}\n","    grads = {'dbeta': dbeta}\n","\n","    return params, grads, mean_loss_history"]},{"cell_type":"markdown","metadata":{"id":"Ibgf9AGQQZk-"},"source":["<a name=\"section-5\"></a>\n","\n","## Section 5: Model Evaluation [^](#outline)"]},{"cell_type":"markdown","metadata":{"id":"bXh8JQXzYVZM"},"source":["Having calculated the parameters for our training set, we can predict the labels for the test set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GX7iNaBwCing"},"outputs":[],"source":["## EDIT THIS FUNCTION\n","def predict(X_test, beta):\n","    \"\"\"\n","    This function predicts the labels for the test set.\n","\n","    Arguments:\n","    X_test: Test set of shape (N_test, p+1)\n","    beta: Parameter vector, a numpy array of size p+1\n","\n","    Returns:\n","    y_pred: Vector containing all probabilities of class 1 for X_test\n","\n","    \"\"\"\n","\n","    assert X_test.shape[1] == beta.shape[0]\n","\n","    N_test = X_test.shape[0]\n","    y_pred = initialise(N_test)\n","    beta = beta.reshape(X_test.shape[1], 1)\n","\n","    # Predicting the probabilities\n","    y_log = predict_log(X_test, beta)\n","\n","    y_pred = ...  ## <-- EDIT THIS LINE\n","\n","    return y_pred"]},{"cell_type":"markdown","metadata":{"id":"I9p1-IQcZ_aJ"},"source":["A way to define an algorithm is placing all functions in one model that has all hyperparameters as arguments. This allows you to quickly evaluate different hyperparameters and optimise over these. So, let's do this:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GA_yAaPu90rI"},"outputs":[],"source":["def model(X_train, y_train, X_test, y_test, num_iterations=2000, learning_rate=0.1, print_loss=False):\n","    \"\"\"\n","    This function implements the logistic regression model, starting with initialisation,\n","    then optimisation and finally evaluations on training and testing data.\n","\n","    Arguments:\n","    X_train: Training set of shape (N_train, p+1)\n","    y_train: True label vector of size N_train\n","    X_test: Test set of shape (N_test, p+1)\n","    y_test: True label vector of size N_test\n","    num_iterations: Number of iterations\n","    learning_rate: Step size in updating procedure\n","    print_loss: 'True' to print the mean loss every 100 iterations\n","\n","    Returns:\n","    d: Dictionary containing all the information (mean loss history, y_pred_test, y_pred_train, beta, learning_rate, num_iterations)\n","\n","    \"\"\"\n","\n","    # Initialising parameters with zeros\n","    beta = initialise(X_train.shape[1])\n","\n","    # Gradient descent\n","    parameters, _ , mean_loss_history = optimise(X_train, y_train, beta, num_iterations, learning_rate, print_loss=print_loss)\n","\n","    # Retrieving parameter vector beta from dictionary 'parameters'\n","    beta = parameters['beta']\n","\n","    # Predicting test and training set examples\n","    y_pred_test = predict(X_test, beta)\n","    y_pred_train = predict(X_train, beta)\n","\n","    # Printing train/test accuracy\n","    print('Training accuracy: {} %'.format(100 - np.mean(np.abs(y_pred_train - y_train)) * 100))\n","    print('Test accuracy: {} %'.format(100 - np.mean(np.abs(y_pred_test - y_test)) * 100))\n","\n","    # Saving all the information\n","    d = {'mean_loss_history': mean_loss_history, 'y_pred_test': y_pred_test, 'y_pred_train': y_pred_train, 'beta': beta, 'learning_rate': learning_rate, 'num_iterations': num_iterations}\n","\n","    return d"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u60bSkU790hY"},"outputs":[],"source":["# Running the model\n","d = model(X_train, y_train, X_test, y_test, num_iterations=500, learning_rate=0.1, print_loss=True)"]},{"cell_type":"markdown","metadata":{"id":"xFQ3dtktpuR6"},"source":["Let's see how our loss has changed over the training iterations:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6wa04Wc690c6"},"outputs":[],"source":["loss_history = np.squeeze(d['mean_loss_history'])\n","plt.ylabel('Mean training loss')\n","plt.xlabel('Iterations (in hundreds)')\n","plt.title('Learning curve for learning rate = ' + str(d['learning_rate']))\n","plt.plot(loss_history);"]},{"cell_type":"markdown","metadata":{"id":"9etWossCprpL"},"source":["#### Questions:\n","1. What insights do you gain from this learning curve?\n","2. Try different learning rates, run the model again, and plot the learning curve. What can you observe?\n","3. Use different random states when you generate the data and run the model again. What can you observe?\n","4. Increase the number of features in your generated data and evaluate the accuracies again. How do they change?\n","5. Generate data with sklearn's [`make_moons`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html) function (noise $> 0.1$) and evaluate how well logistic regression performs on this dataset.\n","6. By performing logistic regression, we have estimated the parameter vector $\\boldsymbol \\beta$ that defines a decision boundary. This is illustrated in the cell below. What does this boundary (the dashed line) mean?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aUsUqWrRw7yK"},"outputs":[],"source":["beta = np.squeeze(d['beta'])\n","\n","x1 = np.asarray([-2,2])\n","m = -beta[1]/beta[2]\n","c = -beta[0]/beta[2]\n","x2 = m*x1 + c\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.scatter(X[:,1], X[:,2], c=y.reshape(-1), cmap='bwr');\n","plt.plot(x1, x2, 'm--', lw=2)\n","plt.title('2-class dataset with boundary')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"m40flsHdjZBT"},"source":["<a name=\"extra-1\"></a>\n","\n","## Extra 1: Receiver Operating Characteristic (ROC) [^](#outline)\n","\n","After obtaining $h_{\\boldsymbol \\beta}(\\boldsymbol x^{(i)}) \\in [0,1]$, we use a threshold $\\tau$ to make binary classification, namely if $h_{\\boldsymbol \\beta}(\\boldsymbol x^{(i)})>\\tau \\implies x^{(i)} \\in \\{1\\}$.\n","\n","Now we can raise interesting questions about accuracy as a metric that we have already used. In the case of $\\tau=0.5$, if the probability is greater than or equal 0.5, then we assign a label of 1 to this example. Consider now two testing examples introduced to the model $\\boldsymbol x^{(i)}$ and $\\boldsymbol x^{(j)}$ both sampled with label 1, and the model produces as probabilities $h_{\\boldsymbol \\beta}(\\boldsymbol x^{(i)}) = 0.49$ and  $h_{\\boldsymbol \\beta}(\\boldsymbol x^{(j)}) = 0.001$. After rounding, using accuracy as a metric will treat both examples equally as false negatives. However, this would neglect the closeness of $h_{\\boldsymbol \\beta}(\\boldsymbol x^{(i)}) = 0.49$ to the decision boundary, as opposed to $h_{\\boldsymbol \\beta}(\\boldsymbol x^{(j)}) = 0.001$.\n","\n","\n","Receiver Operating Characteristic (ROC) curve aims to capture the model output probabilities (or scores) into the quantification of the model performance. In lecture notes, you have learned that ROC curve can be constructed by computing True Positive Rate (TPR) and False Positive Rate (FPR) for a range of thresholds. An ideal model would have Area Under the Curve (AUC) of 1, while a model with random decisions would have an AUC of 0.5.\n","\n","Let's consider the following procedure to construct a ROC curve:\n","\n","\n","1. Define the set of thresholds. This can be uniformly spaced thresholds between 1 and 0. However, the number of threshold values to consider remains a parameter. Alternatively, we may consider only the distinct probability values in `y_log`.\n","> For example, if the computed `y_log` for four-examples is `[0.8, 0.4, 0.9, 0.8]`, then we have `{0.4, 0.8, 0.9}` as distinct values. If we scan the range of thresholds from 1 to 0 using very small steps, we can see that the FPR and TPR will remain unchanged unless we cross these values `{0.9, 0.8, 0.4}`. This means we can just use the distinct values in `y_log` (sorted in decreasing order) as threshold values to evaluate the list of TPR and FPR that we need to construct the ROC curve.\n","2. Starting from the greatest to the least threshold, compute the TPR and FPR and store them in an array or list.\n","3. At this point, AUC can be computed using the Trapezoidal rule (see [numpy.trapezoid](https://numpy.org/doc/stable/reference/generated/numpy.trapezoid.html#numpy-trapezoid)).\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aJdlEVO4EMOl"},"outputs":[],"source":["def roc_curve(y_true, y_log):\n","    \"\"\"\n","    This function plots the ROC curve and computes the AUC.\n","\n","    Arguments:\n","    y_true: Ground truth labels with size N\n","    y_log: Probabilities produced by logistic regression model with size N\n","\n","    Returns:\n","    auc: Area Under the Curve (AUC)\n","    tpr_l: List of true positive rate (TPR) values for each scanned threshold\n","    fpr_l: List of false positive rate (FPR) values for each scanned threshold\n","    thresholds: Scanned thresholds sorted in decreasing order\n","\n","    \"\"\"\n","    # List of distinct values in y_log, sorted sorted in decreasing order\n","    thresholds = ...  ## <-- EDIT THIS LINE\n","    tpr_l, fpr_l = [], []\n","\n","    for threshold in thresholds:\n","        # Thresholding\n","        y_thresholded = (y_log >= threshold)\n","\n","        # True positives\n","        tp = np.sum(y_true & y_thresholded)\n","        # True negatives\n","        tn = np.sum((~y_true) & (~y_thresholded))\n","        # False positives\n","        fp = np.sum((~y_true) & y_thresholded)\n","        # False negatives\n","        fn = np.sum(y_true & (~y_thresholded))\n","\n","        tpr = ...  ## <-- EDIT THIS LINE\n","        fpr = ...  ## <-- EDIT THIS LINE\n","        tpr_l.append(tpr)\n","        fpr_l.append(fpr)\n","\n","    # Compute AUC using Trapezoidal rule\n","    auc = np.trapezoid(tpr_l, fpr_l)\n","    return auc, tpr_l, fpr_l, thresholds"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iMlbiTjR_GSs"},"outputs":[],"source":["y_log = predict_log(X_test, d['beta'])\n","auc, tpr, fpr, th = roc_curve(y_test, y_log)\n","\n","plt.title(f'Receiver Operating Characteristic')\n","plt.plot(fpr, tpr, 'b', label=f'AUC = {auc:0.2f}', lw=2.5)\n","plt.plot([0, 1], [0, 1], 'r--', label='Identity function')\n","plt.legend(loc='lower right')\n","plt.xlim([0, 1])\n","plt.ylim([0, 1])\n","plt.ylabel('TPR')\n","plt.xlabel('FPR')\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"OOjfyBbsFT2U"},"source":["#### Questions:\n","1. Compare the AUC you get with `sklearn.metrics.roc_curve` and `sklearn.metrics.auc`.\n","1. Plot the ROC curve for training and test on the same plot. What can you observe?\n","3. Can you replicate your results using [sklearn](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)?\n","6. Based on sklearn's documentation, can you see any differences in the algorithms that are implemented in sklearn?"]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/probml/pyprobml/blob/master/notebooks/intro/logreg.ipynb","timestamp":1600853677509}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":0}